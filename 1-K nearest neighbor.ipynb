{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "K近邻算法(KNN)是一种基本分类与回归的方法,也是经典机器学习中最简单的一种.K近邻法的输入为实例的特征向量,对应于特征空间的点;输出为实例的类别,可以取多类.K近邻法假设给定一个训练数据集,其中的实例类别已定.分类时，对新的实例,根据K各最近邻的训练实例的类别,通过多数表决等方式进行预测.因此,K近邻不具有显示的学习过程.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子:\n",
    "\n",
    "使用KNN对电影题材进行分类\n",
    "\n",
    "电影可以按照题材进行分类,然而题材本身是如何定义的?由谁来判断某部电影属于哪个题材?也就是说同一题材电影有哪些公共的**特征**,比如动作片会有很多的打斗镜头,爱情片会有很多的接吻镜头,恐怖片会有很多的惊悚镜头.但是,不同题材的电影特征也会有交叉,比如动作片也会有爱情片中的镜头(比如接吻),爱情片也会有很多的动作镜头(比如接吻),那么我们应该如何使用现有的特征对电影进行题材分类？\n",
    "\n",
    "下面这个例子中含有6部电影的镜头数据集:\n",
    "\n",
    "![](picture/36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在表中 **of kicks,of kisses**为特征(futures),**Type of movie**为标签(labels).\n",
    "\n",
    "**注意:**\n",
    "\n",
    "- 一般如果需要使用标签进行学习的话,那么我们可以称之为**监督学习**,如果算法学习不需要标签,我们称之为**非监督学习**.由于KNN比较特殊,我们可以将其归类为监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中,我们如何使用现有的数据集将新进来的未知电影名进行题材分类?\n",
    "\n",
    "![](picture/37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如果要使用KNN算法将上述案例中的电影进行分类,那么我们需要先看看KNN算法的核心是如何进行的,实际上KNN算法的核心很简单即为**距离度量**\n",
    "\n",
    "### 1-距离度量:\n",
    "\n",
    "特征空间中两个实例点的距离是两个实例**相似度的反映**.KNN模型的特征空间一般是n维实数向量空间$R^n$.使用的距离是欧式距离,但是也可以是其他距离,如更一般的$L_p$距离($L_p$ distance),或Minkowski距离.\n",
    "\n",
    "特征空间X是n维实数向量空间$R^n$,$x_i,x_j \\in X,x_i=(x_i^{(1)},x_i^{(2)},x_i^{(3)},...x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},x_j^{(3)},...x_j^{(n)})^T,x_i,x_j$的$L_p$距离定义为 \n",
    "\n",
    "### $L_p(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 $p\\geqslant 1$. P=2时,称为欧式距离(Euclidean distance),即\n",
    "\n",
    "### $L_2(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2)^{\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当P=1,为曼哈顿距离(Manhattan distance),即\n",
    "\n",
    "### $L_1(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$P=\\infty $,它是各个坐标距离的最大值,即\n",
    "\n",
    "### $L_{\\infty}(x_i,x_j)=\\underset{l} max |x_i^{(l)}-x_j^{(l)}|$\n",
    "\n",
    "**注意:**\n",
    "\n",
    "- 不同的距离度量所确定的邻近点是不同的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-K的选择:\n",
    "\n",
    "K值的选择会对结果产生重大的影响,所以在KNN中,K就是一个需要人为调节的参数\n",
    "- K选择的太小,会在较小的领域中进行选择,这样容易发生过拟合,比如k=1时,为最近邻算法.\n",
    "- K选择的太大,同样会产生欠拟合\n",
    "- K值最好选择的是奇数个,容易在K个结果中去\"投票\"\n",
    "- K值的选择一般比较小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- KNN Algorithm\n",
    "\n",
    "下面给出KNN算法\n",
    "\n",
    "**KNN:**\n",
    "\n",
    "输入:训练样本集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$\n",
    "\n",
    "\n",
    "其中,$x_i \\in X \\subseteq R^n$为实例的特征向量,$y_i \\in Y=\\{c_1,c_2,...,c_k\\}$为实例的类别,$i=1,2...N$;实例特征向量x;\n",
    "\n",
    "\n",
    "输出:\n",
    "\n",
    "\n",
    "实例x所属的类y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 根据给定的距离度量,在训练集T中找出与x最近邻的k个点,涵盖着K个点的x的领域记为$N_k(x)$;\n",
    "\n",
    "(2) 在$N_k(x)$中根据决策规则(如多数表决)决定x的类别y;\n",
    "\n",
    "### $y=\\underset{c_j} argmax \\sum_{x_i \\in N_k(x)} I(y_i=c_i),i=1,2,...N; j=1,2,..K$\n",
    "\n",
    "其中$I$为指示函数,即当$y_i=c_j$时$I$为1，否则为0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-电影分类代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    \"\"\"\n",
    "    loading data.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "       1. 在机器学习中,我们经常使用的是向量的形式.\n",
    "       2. 由于KNN中labels不参与计算,所以我们可以将labels定义为字符形式,一般在其他情况下,我们会将Y定义为1,0或者1,-1的形式在二分类中.\n",
    "       \n",
    "    Return:\n",
    "    -------\n",
    "        trainset and labels\n",
    "    \n",
    "    \"\"\"\n",
    "    X = [[3,104],\n",
    "        [2,100],\n",
    "        [1,81],\n",
    "        [101,10],\n",
    "        [99,5],\n",
    "        [98,2]]\n",
    "    \n",
    "    Y = ['Romance','Romance','Romance','Action','Action','Action']\n",
    "    \n",
    "    return np.array(X),np.array(Y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X,y,data,K):\n",
    "    \"\"\"\n",
    "    将传入未知电影的特征与现有的训练特征进行KNN计算.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        X:trainset\n",
    "        y:labels\n",
    "        data: test set\n",
    "        K:parameter K in KNN model.\n",
    "        \n",
    "    Return:\n",
    "    ------\n",
    "        classify result.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate euclidean distance\n",
    "    Euclidean_distance = np.power(np.sum((data - X)**2,axis=1),0.5)\n",
    "    \n",
    "    # get \"short distance\" labels.\n",
    "    sort_k = np.argsort(Euclidean_distance)[:K]  # real number of k.\n",
    "    get_K_y = y[sort_k]\n",
    "    \n",
    "    \n",
    "    # calculate label Probability in get_K_y.\n",
    "    prob_dict = {}\n",
    "    \n",
    "    for label in get_K_y:\n",
    "        if label not in prob_dict:\n",
    "            prob_dict[label] = 1\n",
    "        else:\n",
    "            prob_dict[label] += 1\n",
    "    \n",
    "    predict_y = sorted(prob_dict.items(),key=lambda z:z[1],reverse=True)[0]\n",
    "    \n",
    "    print('predict result is {} and the probability is {}.'.format(predict_y[0],predict_y[1] / len(get_K_y)))\n",
    "    \n",
    "    return predict_y[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict result is Romance and the probability is 1.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Romance'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[18,90]])\n",
    "KNN(X,y,data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到,在使用KNN的时候,并不存在学习参数,而是每次有新进来的数据都需要计算距离,这对于小样本而言是无伤大碍的,但是对于大数据集而言就显得计算量过大,比如10张高清图片."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-约会网站测试KNN\n",
    "\n",
    "现在给予一份约会网站的数据,按照已有的数据集对VIP进行人选的推荐.\n",
    "\n",
    "#### 5.1 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    \"\"\"\n",
    "    load data\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "        1.Train_data is numpy.\n",
    "        2.labels; as same as numpy.\n",
    "    \"\"\"\n",
    "    Train_data = []\n",
    "    labels = []\n",
    "    np.random.shuffle\n",
    "    \n",
    "    with open(path) as f:\n",
    "        \n",
    "        original_data = f.readlines()\n",
    "        \n",
    "    for data in original_data:\n",
    "        data_split = data.strip().split('\\t')\n",
    "        train_ = data_split[:-1]\n",
    "        label_ = data_split[-1]\n",
    "        Train_data.append(train_)\n",
    "        labels.append(label_)\n",
    "    \n",
    "    \n",
    "    return np.array(Train_data,dtype='float'),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data_set/datingTestSet.txt\"\n",
    "\n",
    "X,y = loadData(path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X与y的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 特征处理\n",
    "\n",
    "在开始建立model之前,我们需要观测一下原数据,在原数据中第一个特征下的值明显高于后面两个特征,这样的数值大小差异会使得对结果的影响或者说权重影响很大,所以我们需要做一些数据预处理操作,也可以叫做[特征工程](https://www.zhihu.com/question/29316149),我们这里使用的方式是归一化,有些地方的英文也叫标准化(normal).\n",
    "\n",
    "归一化:\n",
    "\n",
    "对于每个特征下\n",
    "\n",
    "### $newValue  = \\frac{(oldValue - minValue)}{maxValue - minValue}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(X):\n",
    "    \"\"\"\n",
    "    Normalization data.\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "        newVlue: Normal'data.\n",
    "    \"\"\"\n",
    "    minValue = np.min(X,axis=0)\n",
    "    maxValue = np.max(X,axis=0)\n",
    "\n",
    "    newValue = (X - minValue) / (maxValue - minValue)\n",
    "    \n",
    "    return newValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44832535, 0.39805139, 0.56233353],\n",
       "       [0.15873259, 0.34195467, 0.98724416],\n",
       "       [0.28542943, 0.06892523, 0.47449629],\n",
       "       ...,\n",
       "       [0.29115949, 0.50910294, 0.51079493],\n",
       "       [0.52711097, 0.43665451, 0.4290048 ],\n",
       "       [0.47940793, 0.3768091 , 0.78571804]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Norm_X = normal(X)\n",
    "Norm_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 数据集划分\n",
    "\n",
    "现在数据已经归一化了,这样可以避免权重的影响.那么现在将数据集划分为训练样本和测试样本,实际上我们应该划分的样本应该是训练样本,验证样本与测试样本,由于这里的数据量较小,所以我们只划分训练样本和测试样本,划分数据集的作用是**检测模型的可靠性.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,pre):\n",
    "    \n",
    "    # permutation: shuffling x and return index if you input real number.\n",
    "    shuffle_index = np.random.permutation(X.shape[0])\n",
    "    shuffle_X = X[shuffle_index,:]\n",
    "    shuffle_y = y[shuffle_index]\n",
    "    \n",
    "    split_index = np.int(pre * X.shape[0])\n",
    "    train_x = shuffle_X[:split_index]\n",
    "    train_y = shuffle_y[:split_index]\n",
    "    \n",
    "    test_x = shuffle_X[split_index:]\n",
    "    test_y = shuffle_y[split_index:]\n",
    "    return train_x,train_y,test_x,test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y = split_data(Norm_X,y,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然你也可以使用[scikit-learn](https://scikit-learn.org/)进行数据划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Norm_X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Model\n",
    "\n",
    "现在一切准备工作已经完毕,我们开始着手建立模型,并使用测试集去检测正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_model(X_train, X_test, y_train, y_test,K):\n",
    "    \"\"\"\n",
    "    Implementation KNN model.\n",
    "    \n",
    "    parameters:\n",
    "    ----------\n",
    "        1. X_train: train set.\n",
    "        2. X_test: test set.\n",
    "        3. y_train: train set labels.\n",
    "        4. y_test: test set labels.\n",
    "        5. K: real number, parameter of KNN model.\n",
    "        \n",
    "    Return:\n",
    "    ------\n",
    "        \n",
    "        Test set accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = 0\n",
    "    m,n = X_test.shape  # shape at test set .\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        data = X_test[i]\n",
    "        # calculate euclidean distance\n",
    "        Euclidean_distance = np.power(np.sum((data - X_train)**2,axis=1),0.5)\n",
    "\n",
    "        # get \"short distance\" labels.\n",
    "        sort_k = np.argsort(Euclidean_distance)[:K]  # real number of k.\n",
    "        \n",
    "        get_K_y = y_train[sort_k]\n",
    "        \n",
    "        # calculate label Probability in get_K_y.\n",
    "        prob_dict = {}\n",
    "\n",
    "        \n",
    "        for label in get_K_y:\n",
    "            if label not in prob_dict:\n",
    "                prob_dict[label] = 1\n",
    "            else:\n",
    "                prob_dict[label] += 1\n",
    "\n",
    "        predict_y = sorted(prob_dict.items(),key=lambda z:z[1],reverse=True)[0][0]\n",
    "        \n",
    "        # checking predict y is right?\n",
    "        if predict_y == y_test[i]:\n",
    "            accuracy += 1\n",
    "    \n",
    "    print('The test set accurate is {}'.format(accuracy / len(y_test)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accurate is 0.945\n"
     ]
    }
   ],
   "source": [
    "KNN_model(X_train, X_test, y_train, y_test,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Summary\n",
    "\n",
    "在KNN中需要调控的参数只有K,以及距离的P项(但是在此案例中没有,感兴趣的可以使用不同的距离进行尝试),这是KNN的优势相比于其他模型.然而KNN的缺陷也很明显,那就是每次进入一个新的样本就要与所有的训练样本进行计算,这样也是的计算力度与所耗费的时间太大.所以在日常中我们很少会使用到KNN,但是其思想是非常牛逼的,在后面学到的算法,我们会发现很多都会用到\"距离\"的思想."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现手写数字识别系统.\n",
    "\n",
    "数据集在data_set文件夹下的handwriting.zip\n",
    "\n",
    "Good Luck~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
