{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decisionn Trees\n",
    "![](picture/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if every item in the dataset is in the same class:\n",
    "\n",
    "    if so return the class label\n",
    "    Else\n",
    "        find the best feature to split the data\n",
    "        split the dataset\n",
    "        create a branch node\n",
    "             for each split\n",
    "                 call createBranch and add the result to the branch node\n",
    "             retuen branch node\n",
    "             \n",
    "Please note the recursive nature of createBranch. It calls itself in the second-to-last line.\n",
    "\n",
    "**Note:** some decision trees make a binary split of the data,but we won't do this. We'll follow the ID3 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "\n",
    "Entropy is defined as the expected value of the information. First, we need to define information. If you’re classifying something that can take on multiple values, the information for symbol xi is defined as\n",
    "\n",
    "$l(x_i) = log_{2}p(x_i)$\n",
    "\n",
    "where p(xi) is the probability of choosing this class.\n",
    "\n",
    "To calculate entropy, you need the expected value of all the information of all pos-\n",
    "sible values of our class. This is given by\n",
    "\n",
    "$H = - \\sum_{i=1}^{n}p(x_i)log_{2}p(x_i)$\n",
    "\n",
    "where n is the number of classes.\n",
    "\n",
    "Note: click [here](https://blog.csdn.net/acdreamers/article/details/44661149) to ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "\n",
    "See the data in table 3.1. It contains five animals pulled from the sea and asks if they can survive without coming to the surface and if they have flippers. We would like to classify these animals into two classes: fish and not fish. Now we want to decide whether we should split the data based on the first feature or the second feature. To answer this question, we need some quantitative way of determining how to split the data. We’ll discuss that next.\n",
    "\n",
    "![](picture/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look how to compute SannonENT\n",
    "\n",
    "If we target learnning \"is Fish?\".\n",
    "\n",
    "The table 3.1 total examples is 5, and 2 \"yes\",3 \"no\". So,we compute shannonENT as follows\n",
    "\n",
    "$H = - \\frac{3}{5} log_{2}\\frac{3}{5} - \\frac{2}{5} log_{2} \\frac{2}{5} = 0.9709505944546686$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Function to calculate the Shannon entropy of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    \"\"\"\n",
    "    parameter-- dataSet,it's a dictionary\n",
    "    \n",
    "    return shannon Ent\n",
    "    \"\"\"\n",
    "    numEntrise = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "            labelCounts[currentLabel] += 1\n",
    "        else:\n",
    "            labelCounts[currentLabel] += 1\n",
    "    shannonEnt = 0.\n",
    "#     print(labelCounts)\n",
    "    for key in labelCounts:\n",
    "        # compute shannon entropy \n",
    "        prob = float(labelCounts[key]) /  numEntrise\n",
    "        shannonEnt -= prob * log(prob,2)\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    \"\"\"\n",
    "    create data set with table 3.1 Marine animal data\n",
    "    Note: 1 = Yes ,0= No in fetures.\n",
    "    returns-- data set and labels\n",
    "    \"\"\"\n",
    "    dataSet = [[1,1,'yes'],\n",
    "              [1,1,\"yes\"],\n",
    "              [1,0,\"no\"],\n",
    "              [0,1,\"no\"],\n",
    "              [0,1,\"no\"]]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    return dataSet,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try compute shannonENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "dataSet,labels = createDataSet()\n",
    "shannonENT = calcShannonEnt(dataSet)\n",
    "print(shannonENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Splitting the dataset\n",
    "You just saw how to measure the amount of disorder in a dataset. For our classifier algorithm to work, you need to measure the entropy, split the dataset, measure the entropy on the split sets, and see if splitting it was the right thing to do. You’ll do this for all of our features to determine the best feature to split on. Think of it as a two- dimensional plot of some data. You want to draw a line to separate one class from another. Should you do this on the X-axis or the Y-axis? The answer is what you’re try- ing to find out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet,axis,value):\n",
    "    \"\"\"\n",
    "    Dataset splitting on given feture\n",
    "    \n",
    "    returns : retDataSet\n",
    "    \"\"\"\n",
    "    \n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        # start split dataset\n",
    "        if featVec[axis] == value: # if value of featVec[axis] equal target value, then  append to retDataSet\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no']]\n"
     ]
    }
   ],
   "source": [
    "dataSet,labels = createDataSet()\n",
    "print(dataSet)\n",
    "retDataSet = splitDataSet(dataSet,0,1)\n",
    "print(retDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re now going to combine the Shannon entropy calculation and the splitDataSet() function to cycle through the dataset and decide which feature is the best to split on. Using the entropy calculation tells you which split best organizes your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Choosing the best feature to split on\n",
    "\n",
    "在决策树的每一个非叶子结点划分之前，先计算每一个属性所带来的信息增益，选择最大信息增益的属性来划\n",
    "分，因为信息增益越大，区分样本的能力就越强，越具有代表性，很显然这是一种自顶向下的贪心策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    \"\"\"\n",
    "    choose the best feture to spilt\n",
    "    \n",
    "    return:\n",
    "        bestFeture\n",
    "    \"\"\"\n",
    "    # get Number of index in every feature list.Note: every feature list must be same.\n",
    "    numFeatures = len(dataSet[0]) -1 \n",
    "    # compute \"is Fish\" feature  base entropy,like example in \"dataSet cell\".\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    # set best InfoGain, infoGain is Swedish language~,initialize value equal 0\n",
    "    # and set best feature. initialize value equal -1\n",
    "    bestInfoGain = 0.; bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet] #get every feature list\n",
    "        uniqueVals = set(featList) # unique feature list\n",
    "        newEntropy = 0.\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)\n",
    "            \n",
    "            # follows code about 2 lines, compute shannonEnt.\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "            \n",
    "        infoGain = baseEntropy - newEntropy # comput infoGain\n",
    "        if (infoGain > bestInfoGain): # update infoGain\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i  # update best feature\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "Best Feature index is  0\n"
     ]
    }
   ],
   "source": [
    "dataSet,labels = createDataSet()\n",
    "print(dataSet)\n",
    "bestFeature = chooseBestFeatureToSplit(dataSet)\n",
    "print(\"Best Feature index is \",bestFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,the Best Feature is \"Can survive without coming to surface.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You’ll stop under the following conditions: you run out of attributes on which to split or all the instances in a branch are the same class. If all instances have the same class, then you’ll create a leaf node, or terminating block. Any data that reaches this leaf node is deemed to belong to the class of that leaf node. \n",
    "![](picture/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Befor we create the Decision Tree,we need learning decision tree's logist.\n",
    "\n",
    "Let's look what is decision tree's logist.\n",
    "\n",
    "We using this example dataset\n",
    "\n",
    "![](picture/04.png)\n",
    "\n",
    "\n",
    "- step 1: we need compute Entropy(Fish) = $- \\frac{3}{5} log_{2}\\frac{3}{5} - \\frac{2}{5} log_{2} \\frac{2}{5}=0.9709505944546686$\n",
    "- step 2: comput IG\n",
    "    - No surfacing:\n",
    "        - 1:[is Fish] yes,yes,no \n",
    "        - 0:[is Fish] no,no\n",
    "        - Entropy(1) = $- \\frac{2}{3} log_{2}\\frac{2}{3} - \\frac{1}{3} log_{2} \\frac{1}{3}=0.918296 $\n",
    "        - Entropy(0) = $- 0 - \\frac{1}{2} log_{2} \\frac{1}{2}=0.5 $\n",
    "        - Entropy(No surfacing | Fish) = $\\frac{3}{5}\\times 0.918296 + \\frac{2}{5} \\times 0.5 = 0.750977$\n",
    "        - $\\frac{3}{5},\\frac{2}{5}$: in total dataset\n",
    "        - IG(No surfacing | Fish) = 0.9709505944546686 - 0.750977 = 0.219973\n",
    "    - Flippers:\n",
    "        - 1:[is Fish] yes,yes,no,no\n",
    "        - 0:[is Fish] no\n",
    "        - Entropy(1) = $- \\frac{2}{4} log_{2}\\frac{2}{4} - \\frac{2}{4} log_{2} \\frac{2}{4}=1.0 $\n",
    "        - Entropy(0) = $- 0 - \\frac{1}{2} log_{2} \\frac{1}{1}=0 $\n",
    "        - Entropy(Flippers | Fish) = $\\frac{4}{5}\\times 1 + \\frac{1}{5} \\times 0 = 0.8$\n",
    "        - $\\frac{1}{5},\\frac{4}{5}$: in total dataset\n",
    "        - IG(Flippers | Fish) = 0.9709505944546686 - 0.8 = 0.17095\n",
    "        \n",
    "- step 3: choose best IG(max IG)\n",
    "    - so,we choose feature \"No surfacing\".\n",
    "- step 4: using feature \"No surfacing\" to split dataset.\n",
    "- step 5: repeat like setp 1-3\n",
    "    \n",
    "**Last Note:**\n",
    " - if best feature can't split dataset to  have same class label, then ,can using other feature to split dataset.\n",
    "     - [[1, 1, 'yes'], [1, 0, 'yes'], [1, 0, 'no'], [0, 1, 'yes'], [0, 1, 'no']]\n",
    " \n",
    " - if case have no more features to split, but we also can't split dataset to have same class label, then we use \"majority vote.\"\n",
    "     - [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'yes'], [0, 1, 'no']]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): \n",
    "            classCount[vote] = 0\n",
    "        else:\n",
    "            classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    print(sortedClassCount)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yes', 2), ('no', 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majorityCnt(['yes','yes','yes','no'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tree-building code\n",
    "\n",
    "The first stopping condition is that if all the class labels are the same, then you return this label.\n",
    "\n",
    "The second stopping condition is the case when there are no more features to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet,labels):\n",
    "    # get class list: [\"yes\",\"yes\"..] in last feature.\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    \n",
    "    # define first stopping condition.\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        \n",
    "        return classList[0]\n",
    "    \n",
    "    # define second stopping condition.\n",
    "    if len(dataSet[0]) == 1:\n",
    "        \n",
    "        return  majorityCnt(classList)\n",
    "    \n",
    "    # get best Feature.\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    \n",
    "    # using best feture to get label in labels.\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    \n",
    "    # create result tree.\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    \n",
    "    del (labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues) # get unique values to split dataset.\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:] # Get the remaining tables.\n",
    "        # start Recursive.\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "['no surfacing', 'flippers']\n",
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "dataSet,labels = createDataSet()\n",
    "print(dataSet)\n",
    "print(labels)\n",
    "myTree = createTree(dataSet,labels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yes', 0), ('no', 0)]\n",
      "{'flippers': {0: 'no', 1: {'no surfacing': {0: 'yes', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'yes'], [0, 1, 'no']]\n",
    "labels = ['no surfacing', 'flippers']\n",
    "myTree = createTree(dataSet,labels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best feature can't split dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yes', 0), ('no', 0)]\n",
      "{'no surfacing': {0: {'flippers': {'no': 'no', 'yes': 'yes'}}, 1: {'flippers': {0: 'yes', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "dataSet = [[1, 1, 'yes'], [1, 0, 'yes'], [1, 0, 'no'], [0, 1, 'yes'], [0, 1, 'no']]\n",
    "labels = ['no surfacing', 'flippers']\n",
    "myTree = createTree(dataSet,labels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.using decision trees to predict contact lens type\n",
    "\n",
    "The Lenses dataset3 is one of the more famous datasets. It’s a number of observations based on patients’ eye conditions and the type of contact lenses the doctor prescribed. The classes are hard, soft, and no contact lenses. The data is from the UCI database repository and is modified slightly so that it can be displayed easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingDataSet():\n",
    "    \"\"\"\n",
    "    Implement predict contact lens\n",
    "    returns:\n",
    "        lenesTree\n",
    "    \"\"\"\n",
    "    path = \"data_set/lenses.txt\"\n",
    "    fr = open(path)\n",
    "    lenses = [inst.strip().split('\\t') for inst in fr.readlines() ]\n",
    "    lenesLabels = ['age','prescript','astigmatic','tearRate']\n",
    "    lenesTree = createTree(lenses,lenesLabels)\n",
    "    return lenesTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tearRate': {'normal': {'astigmatic': {'no': {'age': {'pre': 'soft', 'young': 'soft', 'presbyopic': {'prescript': {'myope': 'no lenses', 'hyper': 'soft'}}}}, 'yes': {'prescript': {'myope': 'hard', 'hyper': {'age': {'pre': 'no lenses', 'young': 'hard', 'presbyopic': 'no lenses'}}}}}}, 'reduced': 'no lenses'}}\n"
     ]
    }
   ],
   "source": [
    "lenesTree = loadingDataSet()\n",
    "print(lenesTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Summary\n",
    "\n",
    "The contact lens data showed that decision trees can try too hard and overfit a dataset. This overfitting can be removed by pruning the decision tree, combining adjacent leaf nodes that don’t provide a large amount of information gain.\n",
    "\n",
    "There are other decision tree–generating algorithms. The most popular are C4.5 and CART. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
