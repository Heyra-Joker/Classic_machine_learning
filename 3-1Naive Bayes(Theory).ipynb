{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们现在有一个数据集,它由两类数据组成:\n",
    "\n",
    "![](picture/05.png)\n",
    "\n",
    "如果使用KNN计算,显然计算量非常大,如果我们使用决策树的话,根据数据的复杂度而言,结果也不会非常成功.那么我们可以用朴素贝叶斯的方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 [贝叶斯定理](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86):\n",
    "\n",
    "贝叶斯定理是关于随机事件A和B的条件概率的一则定理:\n",
    "\n",
    "${\\displaystyle P(A|B)={\\frac {P(A)\\times P(B|A)}{P(B)}}}$\n",
    "\n",
    "其中$P(A|B)$是指在事件B发生的情况下事件A发生的概率。\n",
    "\n",
    "在贝叶斯定理中,每个名词都有约定俗成的名称:\n",
    "\n",
    "- $P(A|B)$是已知B发生后A的条件概率,也由于得自B的取值而被称作A的后验概率.\n",
    "- $P(A)$是A的先验概率(或边缘概率),之所以称为\"先验\"是因为它不考虑任何B方面的因素.\n",
    "- $P(B|A)$是已知A发生后B的条件概率,也由于得自A的取值而被称作B的后验概率.\n",
    "- $P(B)$是B的先验概率或边缘概率.\n",
    "\n",
    "也就是说:\n",
    "\n",
    "后验概率 = (似然性(Likelihood)*先验概率)/标准化常量\n",
    "\n",
    "更一般化的情况,假设$\\{A_i\\}$是事件集合里的部分集合,对于任意的$A_i$,贝氏定理可用下式表示:\n",
    "\n",
    "$P(A_{i}|B)={\\frac {P(B|A_{i})\\,P(A_{i})}{\\sum _{j}P(B|A_{j})\\,P(A_{j})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯法是基于贝叶斯定理与**特征条件独立假设**的分类方法.首先基于特征条件的独立假设,学习输入/输出的联合概率分布,然后基于此模型求出**后验概率**的最大y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 后验概率最大化\n",
    "对于训练数据集:\n",
    "\n",
    "$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$\n",
    "\n",
    "其中$x_1,x_2,...,x_N$为样本数据,$Y_1,y_2,...,y_N$为样本的labels,假设有$K$类.\n",
    "\n",
    "现在我们的目标是在给定的数据集$T$,依靠$X$去预测$Y$记为$C=\\{c_1,...,c_N\\}$,即:\n",
    "\n",
    "$C=P(Y|X)=\\frac{P(x_i|y_i)P(y_i)}{\\sum_{n=1}^{N}P(x_i|y_i)P(y_i)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**那么Naive Bayes的思想就是将实例分到后验概率最大的类中.**\n",
    "\n",
    "选择0-1损失函数:\n",
    "\n",
    "$L(Y,f(x))=\\left\\{\\begin{matrix}\n",
    "1, & Y \\neq  f(x); \\\\ \n",
    " 0,& Y =f(x) \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "其中$Y$:True labels,$f(x)$:Predict labels\n",
    "\n",
    "也就是说,我们要尽可能的预测对,让$L(Y,f(x))=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期望风险函数为:\n",
    "\n",
    "$R_{exp}(f)=E_x\\sum_{n=1}^{N}[L(L,f(x))]P(c_n|X)$\n",
    "\n",
    "现在为了使得期望风险最小化(损失最小化):\n",
    "\n",
    "$f(x)=\\underset{y}{argmin}\\sum_{n=1}^{N}L(c_k,y)P(c_k|X=x)$\n",
    "\n",
    "$=\\underset{y}{argmin}\\sum_{n=1}P(y \\neq c_k|X=x)$:要让分类错误的最小\n",
    "\n",
    "$=\\underset{y}{argmin}(1-P(y = c_k|X=x))$\n",
    "\n",
    "$=\\underset{y}{argmax}P(y=c_k|X=x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以最小化损失函数值就相当于最大化后验概率$P(y=c_k|X=x)$,这也就是Naive Bayes的原理.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "后验概率最大化(极大化),是很多高阶算法的基础思想."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 求得后验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于训练数据集:\n",
    "\n",
    "$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$\n",
    "\n",
    "我们要求得后验概率:\n",
    "\n",
    "$C=P(Y|X)=\\frac{P(x_i|y_i)P(y_i)}{\\sum_{n=1}^{N}P(x_i|y_i)P(y_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先求先验分布:\n",
    "\n",
    "$P(Y=c_k),k=1,2,...,N$\n",
    "\n",
    "先验分布的概率很简单,因为数据样本的true labels已经有了.\n",
    "\n",
    "那么重点就是后验分布:\n",
    "\n",
    "$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(N)}=x^{(N)}|Y=c_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个后验分布$P(X=x|Y=c_k)=P(X^{(1)}$是有指数数量级的参数,直接求是没有办法的.但是由于Naive Bayes中最重要的思想:\n",
    "\n",
    "**条件独立假设**则有:\n",
    "\n",
    "$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(N)}=x^{(N)}|Y=c_k)=\\prod_{j=1}^{N}P(X^{(j)}=x^{(j)}|Y=c_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "- 很明显在一般情况下数据之间是存在关系的,Naive Bayes将数据之间相互独立.所以这里就是为什么叫Naive.\n",
    "- 虽然将数据条件独立会损失一些准确率,但是会使得算法变得简单."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们的后验概率$P(Y|X)$变为:\n",
    "\n",
    "$C=P(Y|X)=\\frac{P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{n=1}^{N}P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是Naive Bayes的基本公式:\n",
    "\n",
    "$C=f(x)=\\underset{c_k}{argmax}\\;\\frac{P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{n=1}^{N}P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Naive Bayes 参数估计\n",
    "\n",
    "应为我们要极大后验概率,实际上我们只要使用极大似然估计就可以了:\n",
    "\n",
    "先验概率的极大似然估计:\n",
    "\n",
    "$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,...,K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件概率$P(X^{(j)}=x^{(j)}|Y=c_k)$:\n",
    "\n",
    "设第$j$个特征$x^{(j)}$的取值可能的集合为$\\{a_{j1},a_{j2},...,a_{jl}\\}$\n",
    "\n",
    "$P(X^{(j)}=a_{jl}|Y=c_{k})=\\frac{\\sum_{i=1}^{N}I(x_{i}^{j}=a_{jl},y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_i^{(j)}$是第$i$个样本的第$j$个特征;$a_{jl}$是第$j$个特征的可能取值$l$;$I$为指示函数\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "[何为极大似然估计?](7-1EM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Naive Bayes MLE 推导\n",
    "\n",
    "设数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$其中$x\\in R^{d}$且第$j$个分量$x^{(j)}$取值为${a_{j1},a_{j2},...,a_{jS_j}}$共$s_j$个.$y$为类标签.共$K$类即$y$的取值为${c_1,c_2,...,c_K}$\n",
    "\n",
    "对于第$i$个数据$(x_i,y_i)$有$P(X=x_i,Y=y_i)=P(X^{(1)}=x_{i}^{(1)},...,X^{(d)}=x_{i}^{(d)},Y=y_i)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有联合概率可以得到:\n",
    "\n",
    "$P(X=x_i,Y=y_i)=P(X^{(1)}=x_{i}^{(1)},...,X^{(d)}=x_{i}^{(d)},Y=y_i)=\\prod_{j=1}^{d}P(X^{(j)}=x^{(j)}_i|Y=y_i)\\cdot P(Y=y_i)$\n",
    "\n",
    "所以似然函数即为:\n",
    "\n",
    "$L(\\theta)=\\prod_{i=1}^{N}P(X=x_i,Y=y_i)=\\prod_{i=1}^{N}\\prod_{j=1}^{d}P(X^{(j)}=x_{i}^{(j)}|Y=y_i)\\cdot P(Y=y_i)$其中$\\theta$为所有参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取对数$L(\\theta)$:\n",
    "\n",
    "$L(\\theta)=\\sum_{i=1}^{N}logP(Y=y_i)+\\sum_{i=1}^{N}\\sum_{j=1}^{d}logP(X^{(j)}=x_i^{(j)}|Y=y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.先求先验概率$P(Y=c_{k}):$**\n",
    "\n",
    "令$\\alpha_{k}=P(Y=c_{k})$\n",
    "\n",
    "所以:\n",
    "\n",
    "$P(Y=y_i)=\\prod_{k=1}^{K}\\alpha_{k}^{I(y_i=c_{k})}$\n",
    "\n",
    "其中$I(y_i=c_{k})$为指示函数:\n",
    "\n",
    "$I(y_i=c_{k}) = \\left\\{\\begin{matrix}\n",
    "0 &y_i\\neq c_k \\\\ \n",
    " 1&y_i=c_k \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "举个简单的例子:比如对于某个$i$样本,对于当前$y_i$搜索所有的$c_k$如果$y_i=c_{k}$,那么对应的概率$P(Y=y_i)$存在,否则为1,对其无影响.\n",
    "\n",
    "那么对其$\\alpha_{k}$有约束:$\\sum_{k}^{K}\\alpha_{k}=1$,也就是说所有的$y_i$的概率之和为1.\n",
    "\n",
    "所以我们可以采用[拉格朗日乘子法](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0),且由于$L(\\theta)$中的第二项与$\\alpha_k$无关,不做考虑.\n",
    "\n",
    "$L(\\alpha_k,\\lambda)=\\sum_{i=1}^{N}log(\\prod_{k=1}^{K}\\alpha_{k}^{I(y_i=c_{k})})+\\lambda(\\sum_{k}^{K}\\alpha_{k} -1)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}I(y_i=c_{k})\\cdot log\\alpha_k+\\lambda(\\sum_{k}^{K}\\alpha_{k} -1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$\\alpha_k$求偏导:\n",
    "\n",
    "$\\frac{\\partial L(\\alpha_k,\\lambda)}{\\partial \\alpha_k}=\\sum_{i=1}^{N}I(y_i=c_k)\\frac{1}{\\alpha_k}+\\lambda=0$\n",
    "\n",
    "$\\Rightarrow $\n",
    "\n",
    "$\\alpha_k=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{-\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对于$\\alpha_k$求偏导,由于只是对某一个$k$求偏导,所以在$\\sum_{k=1}^{K}$中只有某一个$k$能够作为变量求偏导,其他的$k$即为常数,所以在求偏导完成后就没有$\\sum_{k=1}^{K}$项了.\n",
    "\n",
    "那么对于约束条件$\\sum_{k=1}^{K}=1$有:\n",
    "\n",
    "$\\frac{\\sum_{k=1}^{K}\\sum_{i=1}^{N}I(y_i=c_k)}{-\\lambda}=1$\n",
    "\n",
    "\n",
    "$\\Rightarrow $\n",
    "\n",
    "$\\lambda=-N$ \n",
    "\n",
    "\n",
    "\n",
    "所以:\n",
    "\n",
    "$\\alpha_k=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}$\n",
    "\n",
    "即:\n",
    "\n",
    "$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N}$\n",
    "\n",
    "也就是说,以前我们比如遇到这样的一个问题:\n",
    "\n",
    "$Y=(1,2,3)$求1所占的概率$P(Y=1)$为什么是$\\frac{1}{3}$\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "(1) $\\sum_{k=1}^{K}\\sum_{i=1}^{N}I(y_i=c_k)$实际上就是将整体$K$个特征下的$N$个特征全部走一遍,那么总和即为$N$\n",
    "\n",
    "举个例子:\n",
    "\n",
    "比如$(x^{(1)}_1=1,x^{(2)}_1=L,x^{(1)}_2=1,x^{(2)}_2=M,x^{(1)}_3=0,x^{(2)}_3=S),(y_1=1,y_2=-1,y_3=1),C={1,-1}$\n",
    "\n",
    "那么当$k=1$的时候即$c_k=1$,那么对于$i:1\\rightarrow3$,指示产生的结果是$1,0,1$\n",
    "\n",
    "当$k=2$的时候即$c_k=-1$,那么对于$i:1\\rightarrow3$,指示函数产生结果是$0,1,0$\n",
    "\n",
    "那么求和即为$N=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.求条件概率$P(x^{(j)}=a_{jl}|Y=c_k)$**\n",
    "\n",
    "其中$j=1,2,...d,l=1,...,s_{j},k=1,...,K$\n",
    "\n",
    "我们令$P(x^{(j)}=a_{jl}|Y=c_k)=\\beta_{lk}$\n",
    "\n",
    "求条件概率我们需要知道其联合概率,所以:\n",
    "\n",
    "那么对于联合概率(联合分布)$P(x^{(j)}=a_{jl},Y=c_k)$:\n",
    "\n",
    "$P(x^{(j)}=a_{jl},Y=c_k)=P(x^{(j)}=a_{jl}|Y=c_k)\\cdot P(Y=c_k)=\\beta_{lk}\\cdot \\alpha_{k}$\n",
    "\n",
    "此时我们的似然函数:\n",
    "\n",
    "$L(\\theta)=\\prod_{i=1}^{N}\\prod_{j=1}^{d}P(X^{(j)}=x^{(j)}_j,Y=y_i)$.\n",
    "\n",
    "而$P(X^{(j)}=x^{(j)}_j,Y=y_i)=\\prod_{l=1}^{S_{j}}\\prod_{k=1}^{K}(\\beta_{lk}\\cdot \\alpha_{k})^{I(x_{i}^{j}=a_{jl},y_i=c_k)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以:\n",
    "\n",
    "$logL(\\theta)=\\sum_{i=1}^{N}\\sum_{j=1}^{d}\\sum_{l=1}^{s_{j}}\\sum_{k=1}^{K}[I(x_{i}^{j}=a_{jl},y_i=c_k)\\cdot(log\\beta_{lk}+log\\alpha_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$log\\alpha_k$与$\\beta_{lk}$无关,我们可以不考虑\n",
    "\n",
    "则:\n",
    "\n",
    "$l(\\theta)=logL(\\theta)=\\sum_{i=1}^{N}\\sum_{j=1}^{d}\\sum_{l=1}^{s_{j}}\\sum_{k=1}^{K}I(x_{i}^{j}=a_{jl},y_i=c_k)\\cdot log\\beta_{lk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到约束为:\n",
    "\n",
    "对于固定的$j$(每一个特征)有$\\sum_{l=1}^{s_{j}}\\beta_{lk}=1$(也就是说第$i$个样本下的第$j$个特征下的所有可能取值$s_{j}$的概率和为1)\n",
    "\n",
    "比如继续沿用上面的例子:\n",
    "\n",
    "当$c_k=1,j=1$的时候,$s_j={1,0}$,那么将是$\\sum_{l=1}^{s_j}P(x^{(1)}=a_{1l}|Y=1)=\\frac{1}{2}+\\frac{1}{2}=1$\n",
    "\n",
    "所以对于$j=1,2,...,d$那么将有$d$个约束条件\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求$\\beta_{lk}$注意$l$总是落在某个$x^{(j)}$的取值集合中,\n",
    "\n",
    "先取定$j$,那么对应的拉格朗日乘子为$\\lambda_j$\n",
    "\n",
    "则:\n",
    "\n",
    "$L(\\beta_{lk},\\lambda_j)=\\sum_{i=1}^{N}\\sum_{j=1}^{d}\\sum_{l=1}^{s_j}\\sum_{k=1}^{K}I(x_i^{j}=a_{jl},y_i=c_{k})log\\beta_{lk}+\\lambda_{j}(\\sum_{l=1}^{s_j=}\\beta_{lk}-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本$i$对于某个$\\beta_{lk}$求偏导,那么和之前一样$\\sum_{j=1}^{d}\\sum_{l=1}^{s_j}\\sum_{k=1}^{K}$中只有一项是起做用的,所以可以去掉,即:\n",
    "\n",
    "$\\frac{\\partial L(\\beta_{lk},\\lambda_j)}{\\partial \\beta_{lk}}=\\sum_{j=1}^{N}I(x_{i}^{(j)}=a_{jl},y_i=c_k)\\cdot \\frac{1}{\\beta_{lk}}\\cdot\\lambda_j=0$\n",
    "\n",
    "$\\Rightarrow$\n",
    "\n",
    "$\\beta_{lk}=-\\frac{1}{\\lambda_j}\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么根据约束条件:\n",
    "\n",
    "$\\sum_{l=1}^{s_{j}}\\beta_{lk}=-\\frac{1}{\\lambda_j}\\sum_{l=1}^{s_{j}}\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)=1$\n",
    "\n",
    "$\\Rightarrow$\n",
    "\n",
    "$\\lambda_j = - \\sum_{l=1}^{s_{j}}\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某个样本$i$的特征$j$下的所有可能取值的累和实际上就等于$y_i=c_k$的个数,也就是说:\n",
    "\n",
    "$\\sum_{l=1}^{s_{j}}I(x_i^{(j)}=a_{jl},y_i=c_k)$ 等效于 $I(y_i=c_k)$\n",
    "\n",
    "可以参照上面的例子想想.\n",
    "\n",
    "所以:\n",
    "\n",
    "$\\lambda_j = - \\sum_{i=1}^{N}I(y_i=c_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么:\n",
    "\n",
    "$\\beta_{lk}=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "对于数据$X={1,2,3},y={1,0,1}$\n",
    "\n",
    "以前在计算$P(X=1|Y=1)=\\frac{1}{2}$的原理\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Naive Bayes Algorithm\n",
    "\n",
    "下面给出NB的算法\n",
    "\n",
    "**Naive Bayes Algorithm:**\n",
    "\n",
    "输入:训练数据$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$,其中$x_i=(x_i^{1},x_i^{2},...,x_i^{n})^{T}$,$x_{i}^{j}$是第$i$个样本的第$j$个特征,$x_{i}^{j} \\in{a_{j1},a_{j2},...,a_{jl}}$,$a_{jl}$是特征可能的取值;$y_i\\in \\{c_1,...,c_K\\}$;实例$x$\n",
    "\n",
    "输出:实例$x$的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 计算先验概率以及条件概率:\n",
    "\n",
    "\n",
    "$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,...,K$\n",
    "\n",
    "$P(X^{(j)}=a_{jl}|Y=c_{k})=\\frac{\\sum_{i=1}^{N}I(x_{i}^{j}=a_{jl},y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$\n",
    "\n",
    "(2) 计算后验概率即预测结果:\n",
    "\n",
    "$P(Y=c_k) \\prod_{j}P(X^{(j)}=a_{jl}|Y=c_{k})$\n",
    "\n",
    "(3) 确定实例:\n",
    "\n",
    "$C=f(x)=\\underset{c_k}{argmax}\\;\\frac{P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{n=1}^{N}P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}$\n",
    "\n",
    "但是由于我们是选则最大的$c_k$所以我们没有必要计算分母部分,所以最终的形式为:\n",
    "\n",
    "$C=f(x)=\\underset{c_k}{argmax}\\;P(Y=c_k) \\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "有些情况下:如果条件概率接近0(一般情况下不会),这会对后验概率产生影响(因为是$\\prod$),所以我们需要在条件概率中加入一个极小的值$\\epsilon $防止概率为0.则条件概率变为:\n",
    "\n",
    "$P(X^{(j)}=a_{jl}|Y=c_{k})=\\frac{\\sum_{i=1}^{N}I(x_{i}^{j}=a_{jl},y_i=c_k)+\\epsilon}{\\sum_{i=1}^{N}I(y_i=c_k)+S_j\\cdot\\epsilon}$\n",
    "\n",
    "其中$S_j$是$x^{(j)}$的可能取值\n",
    "\n",
    "先验概率变为:\n",
    "\n",
    "$P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)+\\epsilon}{N+K\\epsilon}$\n",
    "\n",
    "这里的极小值$\\epsilon$常取1,这时称之为拉普拉斯平滑(Laplace smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Summary\n",
    "\n",
    "- Naive Bayes的思想实际上很简单,就是需要极大后验概率.从而使得损失函数变小,让数据的预测变的准确.\n",
    "- 在Naive Bayes中不好的地方就是条件独立性假设,虽然会给我们的算法带来简单的计算,但是与此同时也会丢失一些数据之间的信息."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
