{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Classifying with Bayesian decision theory\n",
    "\n",
    "Naive Bayes is a subset of Bayesian decision theory, so we need to talk about Bayesian decision theory quickly before we get to naive Bayes.\n",
    "Assume for a moment that we have a dataset with two classes of data inside. A plot of this data is shown in figure 4.1.\n",
    "\n",
    "![](picture/05.png)\n",
    "\n",
    "We have an equation for the probability of a piece of data belonging to Class 1 (the circles):p1(x,y),and we have an equation for the class belonging to Class 2(the triangles):p2(x,y).\n",
    "To classify a new measurement with features (x,y), we use the following relues:\n",
    "- if p1(x,y) > p2(x,y), then the class is 1.\n",
    "- if p1(x,y) < p2(x,y), then the class is 2.\n",
    "\n",
    "Then if we use kNN, we need do about 1000 distance calculations. if we use decision trees, and make a split if the data once along the x-axis and once along the y-axis. So, the best choice would be the probability comparison we just discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.What is Conditional probability?\n",
    "\n",
    "Let’s spend a few minutes talking about probability and conditional probability. If you’re comfortable with the p(x,y|c1) symbol, you may want to skip this section.\n",
    "\n",
    "![](picture/06.png)\n",
    "\n",
    "Let’s assume for a moment that we have a jar containing seven stones. Three of these stones are gray and four are black, as shown in figure 4.2. If we stick a hand into this jar and randomly pull out a stone, what are the chances that the stone will be gray? There are seven possible stones and three are gray, so the probability is 3/7. What is the probability of grabbing a black stone? It’s 4/7. We write the probability of gray as P(gray). We calcu- lated the probability of drawing a gray stone P(gray) by counting the number of gray stones and dividing this by the total number of stones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the seven stones were in two buckets? This is shown in figure 4.3.\n",
    "\n",
    "![](picture/07.png)\n",
    "\n",
    "If you want to calculate the P(gray) or P(black), would knowing the bucket change the answer? If you wanted to calculate the probabil-ity of drawing a gray stone from bucket B, you could probably figure out how do to that. This is known as conditional probability. We’re calculating the probability of a gray stone, given that the unknown stone comes from bucket B. We can write this as P(gray|bucketB), and this would be read as “the prob- ability of gray given bucket B.” It’s not hard to see that P(gray|bucketA) is 2/4 and P(gray|bucketB) is 1/3.\n",
    "\n",
    "To formalize how to calculate the conditional probability, we can say\n",
    "\n",
    "P(gray|bucketB) = P(gray and bucketB)/P(bucketB)\n",
    "\n",
    "Let’s see if that makes sense: P(gray and bucketB) = 1/7. This was calculated by taking the number of gray stones in bucket B and dividing by the total number of stones. Now, P(bucketB) is 3/7 because there are three stones in bucket B of the total seven stones. Finally,\n",
    "\n",
    "P(gray|bucketB) = P(gray and bucketB)/P(bucketB) = (1/7) / (3/7) = 1/3.\n",
    "\n",
    "**Another useful way to manipulate conditional probabilities is known as Bayes’ rule.**\n",
    " If we have P(x|c) but want to have P(c|x)\n",
    "## $p(c|x) = \\frac{p(x|c)p(c)}{p(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Classifying with conditional probabilities\n",
    "\n",
    "Given a point identified as x,y, what is the probability it came from class c1? What is the probability it came from class c2?.\n",
    "- If P(c1|x, y) > P(c2|x, y), the class is c1. \n",
    "- If P(c1|x, y) < P(c2|x, y), the class is c2.\n",
    "\n",
    "**Note:** naive bayes need \"independence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Classifying text with Python\n",
    "\n",
    "In order to get features from our text, we need to split up the text. But how do we do that? Our features are going to be tokens we get from the text. A token is any combina- tion of characters. You can think of tokens as words, but we may use things that aren’t words such as URLs, IP addresses, or any string of characters. We’ll reduce every piece of text to a vector of tokens where 1 represents the token existing in the document and 0 represents that it isn’t present.\n",
    "\n",
    "To see this in action, let’s make a quick filter for an online message board that flags a message as inappropriate if the author uses negative or abusive language. Filtering out this sort of thing is common because abusive postings make people not come back and can hurt an online community. We’ll have two categories: abusive and not. We’ll use 1 to represent abusive and 0 to represent not abusive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare: making word vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    Create dataset\n",
    "    \n",
    "    returns:\n",
    "        posting list and classVec\n",
    "    \"\"\"\n",
    "    postingList = [['my','dog','has','flea','problems','help','please'],\n",
    "                  ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                  ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless','grabage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] # 1 is absive,0 not\n",
    "    \n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    Create a list of all the unique words in all of our documents.\n",
    "    \n",
    "    return:\n",
    "        vocabSet \n",
    "    \"\"\"\n",
    "    vocabSet = set([]) # create an empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # create the union of two sets\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    \"\"\"\n",
    "    check words exists our vocabulary,1 exists, 0 not.\n",
    "    \n",
    "    return:\n",
    "        returnVec\n",
    "    \"\"\"\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"The word:{} is not in my Vocabulary !\".format(word))\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List0Post =  [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'grabage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "listClasses =  [0, 1, 0, 1, 0, 1]\n",
      "myVocaList =  ['buying', 'mr', 'not', 'dalmation', 'how', 'help', 'is', 'steak', 'ate', 'food', 'stupid', 'love', 'posting', 'my', 'so', 'take', 'I', 'dog', 'flea', 'licks', 'park', 'quit', 'problems', 'has', 'grabage', 'please', 'him', 'maybe', 'cute', 'to', 'stop', 'worthless']\n"
     ]
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "print(\"List0Post = \",listOPosts)\n",
    "print(\"listClasses = \",listClasses)\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print(\"myVocaList = \",myVocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine this list, you’ll see that there are no repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "returnVec = setOfWords2Vec(vocabList=myVocabList,inputSet=listOPosts[0])\n",
    "print(returnVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "returnVec = setOfWords2Vec(vocabList=myVocabList,inputSet=listOPosts[1])\n",
    "print(returnVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "returnVec = setOfWords2Vec(vocabList=myVocabList,inputSet=listOPosts[3])\n",
    "print(returnVec)\n",
    "print(len(returnVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train: calculating probabilities from word vectors\n",
    "\n",
    "Now that you’ve seen how to convert from words to numbers, let’s see how to calculate the probabilities with these numbers. You know whether a word occurs in a document, It’s rewritten here, but I’ve changed the x,y to w. The bold type means that it’s a vector; that is, we have many values, in our case as many values as words in our vocabulary.\n",
    "\n",
    "## $p(c_i|w) = \\frac{p(w|c_i)p(c_i)}{p(w)}$\n",
    "\n",
    "w:it’s a vector\n",
    "\n",
    "i:0,1\n",
    "\n",
    "We’re going to use the right side of the formula to get the value on the left. We’ll do this for each class and compare the two probabilities. How do we get the stuff on the right? We can calculate p(ci) by adding up how many times we see class i (abusive posts or non-abusive posts) and then dividing by the total number of posts. How can we get p(w|ci)? This is where our naïve assumption comes in. If we expand w into individual features, we could rewrite this as p(w0,w1,w2..wN|ci). Our assumption that all the words were independently likely, and something called conditional indepen- dence, says we can calculate this probability as p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci). This makes our calculations a lot easier.\n",
    "\n",
    "**Note:** due to \"set\" has disorder,so the argmax index maybe changed.But we konw the best word is \"stupid\" to split classifly 1(abusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "If we need solving $p(c_1|w)$,then \n",
    "# $p(c_{1}|w) = \\frac{p(w|c_{1})p(c_{1})}{p(w)} = \\frac{p(c_{1})}{p(w)}\\prod p(w_0,w_1,..w_n|c_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the $p(c_1) = p(\\frac{3}{6}) = 0.5$\n",
    "\n",
    "and the $p(w) = p(w_1,w_2..w_n) = p(w_1)p(w_2)\\cdots p(w_n) = p(7/32)p(7/32)\\cdots p(../32)$ in \"independence hypothesis\"\n",
    "\n",
    "Actually, the $p(w)$ is a constant, so we do not care about it.\n",
    "\n",
    "**Very Impotent part** is $\\prod p(w_0,w_1,..w_n|c_1)$\n",
    "\n",
    "**Note:** In the independence hypothesis, the $\\prod p(w_0,w_1,..w_n|c_1) = p(w_0|c_1)p(w_1|c_1)\\cdots p(w_n|c_1)$\n",
    "\n",
    "Imagine, If we have one list call \"c_1_list\", and all elements are under condition \"c_1\"(abusive), then \n",
    "\n",
    "p(w_0|c_1) = First element in the c1_list / Number of c_1_list\n",
    "\n",
    "#### for example:\n",
    "\n",
    "if c_1_list = [1,0,1,0,1,1],then $p(w_0|c_1) = \\frac{1}{6}$,so we can using this method to calculate $p(w_i|c_1)$\n",
    "\n",
    "Finally, we can calculate $\\prod p(w_0,w_1,..w_n|c_1) = p(w_0|c_1)p(w_1|c_1)\\cdots p(w_n|c_1)$\n",
    "\n",
    "**Ps:** Due to the loss of precision in the Python, we need using \"$l_n(a)$\".\n",
    "\n",
    " Do we lose anything by using the natural log of a number rather than the number itself? The answer is no.\n",
    " \n",
    "Figure 4.4 plots two functions, f(x) and ln(f(x)). If you examine both of these plots, you’ll see that they increase and decrease in the same areas, and they have their peaks in the same areas. Their values are different, but that’s fine. \n",
    "\n",
    "![](picture/08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So,\n",
    "the $p(c_{1}|w)$ will be changed as follows as\n",
    "- do not care about(p(w)), because the p(w) is a constant.\n",
    "    - $l_{n}(p(c_{1}|w)) = l_{n}(p(w|c_{1})p(c_{1})) = l_{n}p(w|c_i) + l_{n}p(c_1) =l_{n}p(w_1|c_1) + l_{n}p(w_2|c_1) +\\cdots + p(w_n|c_1) + l_{n}p(c_1) $\n",
    "    \n",
    "- In the code:\n",
    "    - sum(list of the $p(w|c_1)$) + log($p(c_1)$\n",
    "    \n",
    "**PPs:** \n",
    "\n",
    "If, do not use \"$l_n()$ function\",this will look something like $p(w_0|c_1)p(w_1|c_1)p(w2|_c1)$.\n",
    "\n",
    "If any of these numbers are 0, then when we multiply them together we get 0. \n",
    "\n",
    "To lessen the impact of this, we’ll initialize all of our occur- rence counts to 1 and initialize the denominators to 2 in the next below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    \"\"\"\n",
    "    create  trainNB0 in this cell.\n",
    "    \n",
    "    returns:\n",
    "        p0Vect: probability vectors with classify 0\n",
    "        p1Vect: probability vectors with classify 1\n",
    "        pAbusive: probability abusive for input documents.\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = np.sum(trainCategory) / float(numTrainDocs) # two class problem calculate c_1, c_0 = 1 - c_1\n",
    "    \n",
    "    # initialize\n",
    "    p0Num = np.zeros((1,numWords))\n",
    "    p1Num = np.zeros((1,numWords))\n",
    "    p0Denom = 0.\n",
    "    p1Denom = 0.\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        # create \"condition c_1_list\" or \"condition c_0_list\"\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += np.sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i])\n",
    "    \n",
    "    # calculate p(w_i|c_i)\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pAb: 0.5\n",
      "p0V: [[0.         0.04166667 0.         0.04166667 0.04166667 0.04166667\n",
      "  0.04166667 0.04166667 0.04166667 0.         0.         0.04166667\n",
      "  0.         0.125      0.04166667 0.         0.04166667 0.04166667\n",
      "  0.04166667 0.04166667 0.         0.         0.04166667 0.04166667\n",
      "  0.         0.04166667 0.08333333 0.         0.04166667 0.04166667\n",
      "  0.04166667 0.        ]]\n",
      "p1V: [[0.05263158 0.         0.05263158 0.         0.         0.\n",
      "  0.         0.         0.         0.05263158 0.15789474 0.\n",
      "  0.05263158 0.         0.         0.05263158 0.         0.10526316\n",
      "  0.         0.         0.05263158 0.05263158 0.         0.\n",
      "  0.05263158 0.         0.05263158 0.05263158 0.         0.05263158\n",
      "  0.05263158 0.10526316]]\n",
      "argmax p1V:  10\n"
     ]
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)\n",
    "print(\"pAb:\",pAb)\n",
    "print(\"p0V:\",p0V)\n",
    "print(\"p1V:\",p1V)\n",
    "print(\"argmax p1V: \",np.argmax(p1V[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you found the probability that a document was abusive: pAb; this is 0.5, which is correct. Next, you found the probabilities of the words from our vocabulary given the document class. Let’s see if this makes sense. The first word in our vocabulary is \"dalmation\". This appears once in the 0 class and never in the 1 class. The probabilities are 0.04166667 and 0.0. This makes sense. Let’s look for the largest probability. That’s 0.15789474 in the P(1) array at index 21. If you look at the word in myVocabList at index 14, you’ll see that it’s the word \"stupid\". \n",
    "\n",
    "**This tells you that the word stupid is most indicative of a class 1 (abusive).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we change \"np.zeros\" to \"np.ones\",using \"log\" and Denom equal 2 in initialize part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMatrix,trainCategory):\n",
    "    \"\"\"\n",
    "    create trainNB and changed some code.\n",
    "    returns:\n",
    "        p0Vect: probability vectors with classify 0\n",
    "        p1Vect: probability vectors with classify 1\n",
    "        pAbusive: probability abusive for input documents.\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = np.sum(trainCategory) / float(numTrainDocs) # two class problem calculate c_1, c_0 = 1 - c_1\n",
    "    \n",
    "    # initialize\n",
    "    p0Num = np.ones((1,numWords))\n",
    "    p1Num = np.ones((1,numWords))\n",
    "    p0Denom = 2.\n",
    "    p1Denom = 2.\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        # create \"condition c_1_list\" or \"condition c_0_list\"\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += np.sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i])\n",
    "    \n",
    "    # calculate p(w_i|c_i)\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    \"\"\"\n",
    "    create classifyNB.\n",
    "    returns:\n",
    "        1: abusive\n",
    "        0: not\n",
    "    \"\"\"\n",
    "    #lnp(w1|c1)+lnp(w2|c1)+⋯+p(wn|c1)+lnp(c1)ln(p(c1|w))=ln(p(w|c1)p(c1))=lnp(w|ci)+lnp(c1)=lnp(w1|c1)+lnp(w2|c1)+⋯+p(wn|c1)+lnp(c1)\n",
    "    p1 = np.sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = np.sum(vec2Classify * p0Vec) + np.log(1. - pClass1)\n",
    "    if p1 >p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,PAb = trainNB(np.array(trainMat),np.array(listClasses))\n",
    "    testEntry = [\"love\",\"my\",\"dalmation\"]\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,\"classified as: \",classifyNB(thisDoc,p0V,p1V,PAb))\n",
    "    testEntry = [\"stupid\",\"grabage\"]\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,\"classified as: \",classifyNB(thisDoc,p0V,p1V,PAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'grabage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prepare: the bag-of-words document model\n",
    "\n",
    "\n",
    "Up until this point we’ve treated the presence or absence of a word as a feature. This could be described as a set-of-words model. If a word appears more than once in a document, that might convey some sort of information about the document over just the word occurring in the document or not. This approach is known as a bag-of-words model. A bag of words can have multiple occurrences of each word, whereas a set of words can have only one occurrence of each word. To accommodate for this we need to slightly change the function setOfWords2Vec() and call it bagOfWords2VecMN().\n",
    "The code to use the bag-of-words model is given in the following listing. It’s nearly identical to the function setOfWords2Vec() listed earlier, except every time it encoun- ters a word, it increments the word vector rather than setting the word vector to 1 for a given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # changed \"ginve index\" to  1\n",
    "            # so ,it's calculate number of some word appeared\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4Test: cross validation with naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    \n",
    "    listOfToken = re.split('\\W*',bigString)\n",
    "    return [tok.lower() for tok in listOfToken if len(tok)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    \n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    # load and parse text files,this about 9 lines\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('data_set/email/spam/{}.txt'.format(i),errors=\"ignore\").read())\n",
    "        docList.append(wordList) # create document list like [[\"hello\",\"world\"],[\"hey\",\"name\"]]\n",
    "        fullText.extend(wordList) # full text\n",
    "        classList.append(1) # create classes list it's two-classes\n",
    "        wordList = textParse(open('data_set/email/ham/{}.txt'.format(i)).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    # end of the load and parse text file\n",
    "    \n",
    "    vocabList = createVocabList(docList) # create vocabulary to list like [\"hello\",\"world\",\"hey\",\"name\"]\n",
    "    \n",
    "    # start split traning set and test set,about 6 lines\n",
    "    # this part we can call \"hold-out cross validation\"\n",
    "    trainingSet = list(x for x in range(50))\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0,len(trainingSet))) # Draw samples from a uniform distribution.\n",
    "        testSet.append(trainingSet[randIndex]) # get random int and input to test set.\n",
    "        del(trainingSet[randIndex]) # delete Already extracted numbers.\n",
    "        \n",
    "    # end of the split\n",
    "    \n",
    "    # start training set\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB(np.array(trainMat),np.array(trainClasses))\n",
    "    \n",
    "    # end traning set\n",
    "    \n",
    "    # testing test set\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1 # compute error rate\n",
    "    print(\"The error rate is: \",float(errorCount / len(testSet)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huwang/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the method of \"hold-out cross validation\" used by this code is not particularly good.\n",
    "\n",
    "we can using \"np.random.shuffle()\" and get top 10 to inputing test set, get index of original set.\n",
    "\n",
    "The function spamTest() displays the error rate from 10 randomly selected emails. Since these are randomly selected, the results may be different each time. If there’s an error, it will display the word list for that document to give you an idea of what was misclassified. To get a good estimate of the error rate, you should repeat this proce- dure multiple times, say 10, and average the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Summary\n",
    "\n",
    "Using probabilities can sometimes be more effective than using hard rules for classifi- cation. Bayesian probability and Bayes’ rule gives us a way to estimate unknown proba- bilities from known values.\n",
    "You can reduce the need for a lot of data by assuming conditional independence among the features in your data. The assumption we make is that the probability of one word doesn’t depend on any other words in the document. We know this assump- tion is a little simple. That’s why it’s known as naïve Bayes. Despite its incorrect assumptions, naïve Bayes is effective at classification.\n",
    "There are a number of practical considerations when implementing naïve Bayes in a modern programming language. Underflow is one problem that can be addressed by using the logarithm of probabilities in your calculations. The bag-of-words model is an improvement on the set-of-words model when approaching document classifica- tion. There are a number of other improvements, such as removing stop words, and you can spend a long time optimizing a tokenizer.\n",
    "The probability theory you learned in this chapter will be used again later in the book, and this chapter was a great introduction to the full power of Bayesian probabil- ity theory. We’re going to take a break from probability theory. You’ll next see a classi- fication method called logistic regression and some optimization algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
