{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "逻辑斯蒂回归是一个经典的算法,如果你深入LR你会发现它覆盖的范围特别的广而且它也是现代神经网络的基础.很多高阶算法比如支持向量机(SVM),最大熵模型,能量函数等都和LR有关."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在讲述LR之前,我们要先看看什么叫线性回归.\n",
    "\n",
    "线性回归(Linear regression)就是一个超平面去(或者在一维数据下可以想象成一条直线)拟合样本点的标签如下图所示:\n",
    "\n",
    "![](picture/54.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在假设给定一个点$z$,判断这个点是在图中红线的上方还是下方(简单情况的二分类).计算过程很简单将$z$的点带入到方程$y$即可.如果计算的结果大于0则在直线的上方,若是小于0则在直线的下方.\n",
    "\n",
    "所以:\n",
    "\n",
    "对于样本数据$X=\\{x_1,x_2,...,x_n\\}$,样本标签$Y=\\{y_1,y_2,...,y_n\\}$则预测值有:\n",
    "\n",
    "\n",
    "$f(x_i)= \\sum_{p=1}^{P}w_{p}x_{ip} + b_i=w^{T}x_i+b$\n",
    "\n",
    "其中$w$为权重(weights),$b$为偏置(bias).体现为多个训练样本共同训练$w,b$\n",
    "\n",
    "但是如果以概率方面而言,很明显$f(x)$产生的值会大于1或者小于0.所以我们需要将该直线压缩到概率区间$[0,1]$之内,所以我们就有了二项LR回归(Binomial logistic regression),在工业界也称之为LR.\n",
    "\n",
    "\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "(1) 这里的超平面概念将会在SVM中讨论.\n",
    "\n",
    "(2) 权重和偏置同为拟合参数,权重主要控制超平面的角度而偏置则是超平面的截距.\n",
    "\n",
    "(3) 在后面的讨论中,我们有时候会将偏置放在权重中,以便于讨论的方便.\n",
    "\n",
    "(4) 在神经网络中,很少将偏置和权重放在一起,并且[偏置](https://www.jiqizhixin.com/articles/2018-07-05-18)也起到了非常重要的作用\n",
    "\n",
    "(5) 权重的意思,表示对应的属性在预测结果的权重,这个很好理解,权重越大,对于结果的影响越大\n",
    "\n",
    "(6) 偏置项决定了超平面的平移度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Binomial LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二项LR是工业界一个非常经典的**二分类模型.** \n",
    "\n",
    "它的思想是将线性回归$f(x)$从实数空间$(-\\infty,+\\infty )$使用**Sigmod**函数映射到概率空间$(0,1)$上.\n",
    "\n",
    "那么我们先来看看Sigmod函数:\n",
    "\n",
    "### $\\sigma(z) = \\frac{1}{1+e^{-z}}$ \n",
    "\n",
    "<img src=\"picture/55.png\" width=300 heigth=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出:\n",
    "\n",
    "(1) sigmoid函数的区间大小在$(0,1)$内.\n",
    "\n",
    "(2) sigmoid函数的中值为$0.5$\n",
    "\n",
    "(3) 当$z$趋近于$+\\infty$或者$-\\infty$时,sigmoid的值趋近1或者0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么在二分类下对于概率而言,sigmod函数的性质非常的好,当sigmod的值大于0.5的时候,我们可以说它属于第一类,当sigmoid的值小于0.5的时候,我们可以分类为第二类.\n",
    "\n",
    "其实是和将点带入直线,结果大于0表示在直线上方,小于0表示在直线下方的道理是一样的.直线中结果大于0相当于sigmoid函数中变量大于0的部分,结果小于0相当于sigmoid函数中变量小于0的部分.\n",
    "\n",
    "所以对于二分类:\n",
    "\n",
    "### $P(Y=1|X)=\\frac{1}{1+e^{-Z}} = \\frac{1}{1+e^{-(w\\cdot x+b)}}$\n",
    "\n",
    "### $P(Y=0|X)=1-P(Y=1|X) =\\frac{e^{-(w\\cdot x+b)}}{1+e^{-(w\\cdot x+b)}}$\n",
    "\n",
    "可以看出线性函数$w\\cdot x$的值越接近正无穷,概率值就越接近1,越接近负无穷则概率值越接近0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "更一般的[LR分布](https://baike.baidu.com/item/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%88%86%E5%B8%83),当式子中的位置参数$\\mu=0$,形状参数$\\gamma=1$的时候就是LR.\n",
    "\n",
    "实际上线性回归是不可做分类的,上面的讨论是在前提这条直线是完美确定的情况下,在更一般的情况下不同的拟合直线会导致不同的连续值输出(而我们的分类标签只有0,1),那么对于连续值就需要一个阈值来衡量该将数据分类到0还是1,不过这个阈值会受到奇点的影响.但是Sigmoid函数不会.\n",
    "\n",
    "<img src=\"picture/56.png\" width=400 heigth=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 模型参数\n",
    "\n",
    "LR在学习时,对于给定数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\},y_i\\in\\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$\\hat{y}$为预测值:\n",
    "\n",
    "if $Y=1$: $ P(Y=1|X)= \\hat{y}$\n",
    "\n",
    "if $Y=0$: $ P(Y=0|X)= 1- \\hat{y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整合这两种情况就可以得到似然函数:\n",
    "\n",
    "$\\prod_{i=1}^{N}[\\hat{y_i}]^{y_i}\\cdot[1-\\hat{y_i}]^{1-y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么对数似然函数即为:\n",
    "\n",
    "$L(W,b) = \\sum_{i=1}^{N}[y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么对$L(W,b)$求极大就可以得到其估计值.由于目标是无法直接求出解析解.但是它是高阶连续可微的凸函数(似然函数为指数的product).所以可以使用迭代法,实际上[求优化目标](https://tminka.github.io/papers/logreg/minka-logreg.pdf)的方法都是大同小异,就是找到一个方向,然后朝着这个方向去迭代去逼近最优的参数值,使得似然函数能够减小.\n",
    "\n",
    "迭代的方式这里主要讲2种:梯度下降法(GD,SGD),拟牛顿法(BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 求解最优参数\n",
    "\n",
    "#### 3.1 损失函数\n",
    "\n",
    "损失函数的定义为:\n",
    "\n",
    "$loss(w,b) = -\\frac{1}{N}\\cdot \\sum_{i=1}^{N}[y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出损失函数的形式与对数似然函数很相似,主要区别就是一个负号和$\\frac{1}{N}$:\n",
    "\n",
    "(1) 添加一个负号是因为我们的目标是极大似然函数,那么作为损失函数中\"损失\"的体现,损失越小,似然函数就极大.\n",
    "\n",
    "也就是说极大似然函数$\\underset{w,b}{argmax}\\;L(w,b)$等价于$\\underset{w,b}{argmin}\\;-L(w,b)$\n",
    "\n",
    "(2) 添加$\\frac{1}{N}$是为了将损失值平均一下,进行适当缩放."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 GD,SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法(Gradient Descent)和随机梯度下降法(Stochastic gradient descent).是寻求参数$w,b$比较有效的方法.是对于损失函数求一阶导数(即为梯度),然后使用这个梯度乘上某一个度量长度就得到参数的更新值.\n",
    "\n",
    "假设某个样本$(x_i,y_i)$的参数为$w_i,b_i$则使用梯度下降法的过程如下:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本的线性函数为:\n",
    "\n",
    "$z_i=w_ix_i+b$\n",
    "\n",
    "通过$sigmoid$作用\n",
    "\n",
    "$a_i=sigmoid(z_i)=\\frac{1}{1+e^{-z_i}}$,$a_i$ 等同预测值\n",
    "\n",
    "那么对于参数$w_i$:\n",
    "\n",
    "$loss(w_i,b_i) = -(y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y}))$\n",
    "\n",
    "$dw_i=\\frac{\\partial loss(w_i,b_i)}{\\partial w_i} = \\frac{\\partial loss(w_i,b_i)}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial z_i}\\cdot \\frac{\\partial z_i}{\\partial w_i}$ ①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①式称为链式法则,因为我们要更新的参数$w_i$实际上是在$z_i$内,但是$z_i$又与$a_i$有关且$a_i$又与$loss(w_i,b_i)$有关,就像一条链子连着."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们只要分别求出链式中的各项就可以得到$w_i$\n",
    "\n",
    "先计算$\\frac{\\partial loss(w_i,b_i)}{\\partial a_i} = (\\frac{1-y_i}{1-a_i})-(\\frac{y_i}{a_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在计算$\\frac{\\partial a_i}{\\partial z_i} = a_i(1-a_i)$ 也就是sigmoid的导数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后计算$\\frac{\\partial z_i}{\\partial w_i}=x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以$dw_i=x_i(a_i-y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样对于参数$b_i$:\n",
    "\n",
    "$db_i=a_i-y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就得到了参数的更新值$w_i,b_i$的导数记为$dw_i,db_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1\n",
    "\n",
    "**Gradient Descent algorithm:**\n",
    "\n",
    "输入:样本数据$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\},y_i\\in\\{0,1\\}$,梯度步长$\\alpha$,迭代停止阈值$\\epsilon$\n",
    "\n",
    "输出:样本预测labels.\n",
    "\n",
    "(1) 初始化参数$W,b$,两者都是向量形式,一般定义为一个很小的数字.\n",
    "\n",
    "(2) 计算线性值:$Z=W\\cdot X+b$\n",
    "\n",
    "(3) sigma函数作用:$A=sigmoid(Z)$\n",
    "\n",
    "(4) 计算损失函数:$loss(w,b) = -\\frac{1}{N}\\cdot \\sum_{i=1}^{N}[y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y})]$\n",
    "\n",
    "(5) 更新参数值:$W=W - \\alpha \\cdot dW,b=b-\\alpha \\cdot db$\n",
    "\n",
    "(6) 重复(2)-(5)直到损失函数在阈值$\\epsilon$内.退出迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "- 由于算法是同时针对$N$组样本的,所以:\n",
    "\n",
    "    - $dW=\\frac{1}{m}\\cdot X(A-Y)$\n",
    "\n",
    "    - $db = \\frac{1}{m}\\cdot (A-Y)$\n",
    "\n",
    "    - 来缩放一下梯度值.\n",
    "\n",
    "\n",
    "\n",
    "- 这里的梯度步长$\\alpha$在神经网络中叫做学习率\n",
    "\n",
    "\n",
    "- 除了梯度下降法还有与之相同的叫梯度上升法(Gradient Ascent)\n",
    "    - GA和DG的道理都是一样的,GA的定义是loss函数没有负号(等同于极大似然),最后算出$dW$的结果是$(Y-A)$.\n",
    "    - 更新参数定义为:$W=W+\\alpha \\cdot dW$\n",
    "    - 可以看出sigmoid的结果值$A$:\n",
    "        - if $A<Y$,\n",
    "            - GD:$dW < 0$,$W$增大.\n",
    "            - GA:$dW >0$,$W$增大.\n",
    "        - if $A>Y$,\n",
    "            - GD:$dW > 0$,$W$减小.\n",
    "            - GA:$dW < 0$,$W$减小.\n",
    "    - 因为我们习惯上会希望loss在迭代的过程中变小,所以我们更加习惯使用GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3直观理解GD\n",
    "\n",
    "为什么GD能够在迭代过程中逐渐逼近真实值呢?\n",
    "\n",
    "<img src=\"picture/57.png\" heigth=400 width =400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中红色的点为一类,蓝色的点为另一类,紫色的点表示我们的初始值$w$.\n",
    "\n",
    "下面分两种情况来解释:\n",
    "\n",
    "(1) 如果检测样本是属于(Y=0,也就是红色的部分),那么最优的$w$也是属于红色的,因为$(A-Y)$为正,更新值$W$会减小,随着迭代逐步逼近红色的范围,直到$A$与$Y$极为接近,则$W$更新无变化.\n",
    "\n",
    "(2) 如果检测样本是属于(Y=1,也就是蓝色的部分),那么最优的$w$也是属于蓝色的,因为$(A-Y)$为负,更新值$W$会增加,随着迭代逐步逼近蓝色的范围,直到$A$与$Y$极为接近,则$W$更新无变化.\n",
    "\n",
    "(3) 另外也可以看出初始值的选择不可过大,如果过大或过小,将会进入平缓地带,那么梯度就会下降的十分缓慢."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 SGD\n",
    "\n",
    "实际上SGD的算法跟GD是一样的,只不过每一次迭代只是随机的选取一个样本进行梯度下降,为什么要这样做呢?\n",
    "\n",
    "其实GD每一次迭代都更新了所有的样本参数,这样会有不便之处:\n",
    "\n",
    "(1) 计算量大,对于样本数据而言越大,每次迭代计算量越大.\n",
    "\n",
    "(2) 因为每次迭代都更新所有样本,肯能有些情况下$W$已经是最优的情况了,但是还是需要更新它,那么这一部分就是无效操作.\n",
    "\n",
    "但是SGD也有不好的地方最主要是计算太梳:\n",
    "\n",
    "(1) 因为每次迭代都是随机选取一个,可能有些样本的参数无法到达最优.\n",
    "\n",
    "\n",
    "所以我们一般都会选择折中方案: [mini-Batch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/),将会在神经网络中讲解,所以这里就不多说了.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以图形化解释GD,SGD:\n",
    "\n",
    "<img src=\"picture/63.png\" width=400 heigth=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 拟牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在说明拟牛顿法之前,我们要先了解一下什么叫[牛顿法wiki](https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95)\n",
    "\n",
    "#### 3.3.1 牛顿法\n",
    "\n",
    "假设有一条非线性函数,我们需要求其$f(x)=0$的点.\n",
    "\n",
    "首先，选择一个接近函数 ${\\displaystyle f(x)}$零点的${\\displaystyle x_{0}}$ ,计算相应的${\\displaystyle f(x_{0})}$和切线斜率 ${\\displaystyle f'(x_{0})}$(这里${\\displaystyle f'}$表示函数 ${\\displaystyle f}$ 的导数).然后我们计算穿过点${\\displaystyle (x_{0},f(x_{0}))}$ 并且斜率为${\\displaystyle f'(x_{0})}$ 的直线和 ${\\displaystyle x}$轴的交点的${\\displaystyle x}$坐标，也就是求如下方程的解:\n",
    "\n",
    "${\\displaystyle 0=(x-x_{0})\\cdot f'(x_{0})+f(x_{0})}$\n",
    "\n",
    "我们将新求得的点的 ${\\displaystyle x} $坐标命名为 ${\\displaystyle x_{1}}$,通常 ${\\displaystyle x_{1}}$会比 ${\\displaystyle x_{0}}$ 更接近方程${\\displaystyle f(x)=0}$的解.因此我们现在可以利用 ${\\displaystyle x_{1}}$开始下一轮迭代.迭代公式可化简为如下所示:\n",
    "\n",
    "${\\displaystyle x_{n+1}=x_{n}-{\\frac {f(x_{n})}{f'(x_{n})}}}$ \n",
    "\n",
    "已经证明,如果$f'$是连续的,并且待求的零点$x$是孤立的,那么在零点$x$周围存在一个区域,只要初始值$x_0$位于这个邻近区域内,那么牛顿法必定收敛.并且,如果$f'(x)$不为0,那么牛顿法将具有平方收敛的性能,也就是说意味着每次迭代牛顿法都将有效数字增加一倍.\n",
    "\n",
    "<img src=\"picture/58.gif\" width=400 heigth=400 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[应用于最优化的牛顿法](https://zh.wikipedia.org/wiki/%E6%87%89%E7%94%A8%E6%96%BC%E6%9C%80%E5%84%AA%E5%8C%96%E7%9A%84%E7%89%9B%E9%A0%93%E6%B3%95)\n",
    "\n",
    "牛顿法为解决非线性优化问题提供了一种办法,假设目标是优化一个函数$f$,由求函数极大极小问题,可以转换为求函数$f$的求导$f'=0$的问题.\n",
    "\n",
    "那么转换为$f'=0$之后也就是上面非线性函数求函数值等于0的为题.\n",
    "\n",
    "所以:\n",
    "\n",
    "一维问题的牛顿法主要步骤如下: 取一个点$ {\\displaystyle x_{0}}$为初值,依如下公式迭代:\n",
    "\n",
    "${\\displaystyle x_{n+1}=x_{n}-{\\frac {f^{\\prime }(x_{n})}{f^{\\prime \\prime }(x_{n})}}}\\;\\;\\;\\;①$\n",
    "\n",
    "直至满足一定条件(如${\\displaystyle f^{\\prime }(x_{n})<\\varepsilon}$或 ${\\displaystyle x_{n+1}-x_{n}<\\varepsilon } $, 其中 ${\\displaystyle \\varepsilon } $为一个给定的足够小的常量)后, 算法终止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么①中的迭代公式实际上是一个二阶[泰勒展开](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0)\n",
    "\n",
    "设$f(x)$的二阶泰勒展开为:\n",
    "\n",
    "$f(x+\\nabla{x})=f(x)+f'(x)\\nabla{x}+\\frac{1}{2}\\nabla{x^2}f''(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$\\nabla{x}$求导:\n",
    "\n",
    "### $\\frac{\\partial f(x+\\nabla{x}) }{\\partial f(\\nabla{x})}=f'(x)+\\nabla{x}f''(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$\\Delta{x}\\rightarrow 0$时:\n",
    "\n",
    "$f'(x)+\\Delta{x}f''(x)\\cong 0$\n",
    "\n",
    "$\\Rightarrow $\n",
    "\n",
    "$\\Delta{x}=-\\frac{f'(x)}{f''(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以:\n",
    "\n",
    "${\\displaystyle x_{n+1}=x_{n}-{\\frac {f^{\\prime }(x_{n})}{f^{\\prime \\prime }(x_{n})}}}$\n",
    "\n",
    "那么对于多维的数据,迭代的形式为:\n",
    "\n",
    "${\\displaystyle x_{n+1}=x_{n}-[\\mathbf {H} f(x_{n})]^{-1}\\nabla f(x_{n}),n\\geq 0.}$\n",
    "\n",
    "$\\nabla f(x_{n})$表示$f(x_{n})$的导数,也可以叫做梯度.\n",
    "\n",
    "$\\mathbf {H}$称之为[Hessian矩阵](https://zh.wikipedia.org/wiki/%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5)\n",
    "\n",
    "<img src=\"picture/59.png\" width=400 heigth=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么对于我们的损失函数$loss$是满足最优化牛顿法的.假设$w^{k}$是当前的极小值估计值.那么有:\n",
    "\n",
    "$\\varphi(w)=loss(w^k)+loss'(w^k)(w-w^{k})+\\frac{1}{2}loss''(w^{k})(w-w^{k})^{2}$\n",
    "\n",
    "然后令$\\varphi(w)^{'}=0$,得到$w=w^{k}-\\frac{loss'(w^{k})}{loss''(w^{k})}$\n",
    "\n",
    "因此迭代公式:\n",
    "\n",
    "$w^{k+1}=w^{k}-\\frac{loss'(w^{k})}{loss''(w^{k})}=w^{k}-\\mathbf {H_{k}^{-1}}loss'(w^{k})$\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "这里的$loss$,我将参数$b$放入了$w$矩阵中,形如$w=(b,w_1,w_2,...,w_n)$,因为这样做只需要求一个迭代公式."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然牛顿法一般比GD快,但是在高维数据的情况下,计算Hessian矩阵是很不容易的,而且有时候目标函数无法保证是[正定](https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5)的,就不存在Hessian矩阵的逆.所以就提出了拟牛顿法来近似Hessian矩阵或Hessian矩阵的逆矩阵的正定对称矩阵.不同的构造方法就得到了不同的拟牛顿法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2拟牛顿法\n",
    "\n",
    "假设经过$k+1$次迭代后得到$x_{k+1}$,对此目标函数$f(x)$在$x_{k+1}$附近处做二阶泰勒展开得到:\n",
    "\n",
    "$f(x)\\cong f(x_{k+1})+\\nabla f(x_{k+1})(x-x_{k+1})+\\frac{1}{2}(x-x_{k+1})^{T} \\nabla^{2}f(x_{k+1})(x-x_{k+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上式对$x$求导,得到:\n",
    "\n",
    "$\\nabla f(x) \\cong \\nabla f(x_{k+1})+\\nabla^{2}f(x_{k+1})(x-x_{k+1})=\\nabla f(x_{k+1}) + \\mathbf {H_{k+1}}\\cdot(x-x_{k+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即:\n",
    "\n",
    "$\\nabla f(x) \\cong \\nabla f(x_{k+1}) + \\mathbf {H_{k+1}}\\cdot(x-x_{k+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 $x=x_k$时:\n",
    "\n",
    "$\\nabla f(x_k) - \\nabla f(x_{k+1}) \\cong \\mathbf {H_{k+1}}\\cdot(x_k-x_{k+1})$\n",
    "\n",
    "取反得到:\n",
    "\n",
    "$ \\nabla f(x_{k+1}) -\\nabla f(x_k) \\cong \\mathbf {H_{k+1}}\\cdot(x_{k+1}-x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设:\n",
    "\n",
    "$g_{k+1}=\\nabla f(x_{k+1}),g_k =\\nabla f(x_k)$ 且$\\delta_{k} =x_{k+1}-x_{k},y_k=g_{k+1}-g_{k}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以:\n",
    "\n",
    "$y_k \\cong \\mathbf{H_{k+1}} \\cdot \\delta_{k}$\n",
    "\n",
    "或者\n",
    "\n",
    "$\\delta_{k} \\cong \\mathbf{H_{k+1}^{-1}} \\cdot y_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用$B_{k+1}$来对$H_{k+1}$做近似,用$D_{k+1}$对$\\mathbf{H_{k+1}^{-1}}$做近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到:\n",
    "\n",
    "$y_k \\cong B_{k+1} \\cdot \\delta_{k}$\n",
    "\n",
    "或者\n",
    "\n",
    "$\\delta_{k} \\cong D_{k+1} \\cdot y_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样的话就不用再去求Hessian矩阵了,那么$B_{k+1}$或$D_{k+1}$是如何的呢?对于不同的拟牛顿法算法而言,得到的结果不同.这里就说明最流行的一种算法BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BFGS algorithm:**\n",
    "\n",
    "输入:目标函数$f(x)$,$g(x)=\\nabla f(x)$,精度要求$\\varepsilon$,步长$\\alpha$.\n",
    "\n",
    "输出:$f(x)$的极小点$x^{*}$\n",
    "\n",
    "(1) 给定初始值$x_0$,并令$B_0=I$,迭代次数$R:=0$.\n",
    "\n",
    "(2) 确定搜索方向$d_k=-D_kg_k$,这里的$D_k=B_{k}^{-1}$\n",
    "\n",
    "(3) 计算令$\\delta_{k}=\\alpha_k d_k, x_{k+1}:=x_k+\\delta_{k}$\n",
    "\n",
    "(4) 计算$g_{k+1}$若$||g_{k+1}||<\\varepsilon$,则跳出迭代,获得最优极值点$x_{k+1}=x^{*}$否则\n",
    "\n",
    "(5) 计算$y_k=g_{k+1}-g_{k}$\n",
    "\n",
    "(6) 计算$D_{k+1}=(I-\\frac{\\delta_{k}y_{k}^{T}}{y_{k}^{T}\\delta_{k}})D_{k}(I-\\frac{y_k\\delta_{k}^{T}}{y_{k}^{T}\\delta_{k}})+\\frac{\\delta_{k}\\delta_{k}^{T}}{y^{T}\\delta_{k}}$\n",
    "\n",
    "(7) $R:=R+1$转(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "其实拟牛顿法运用了[凸优化理论](http://59.80.44.98/web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf),想深入了解的可以仔细看看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 多分类\n",
    "\n",
    "上面讲述的是LR的二分类,那么基于二分类可以推广到多分类.\n",
    "\n",
    "假设离散随机变量$Y$的取值集合是$\\{1,2,...,K\\}$那么多分类的LR模型是:\n",
    "\n",
    "$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}$\n",
    "\n",
    "$P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}$\n",
    "\n",
    "参数同理推广\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "由于基于LR的多分类是[One-vs-rest](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB)的,所以我们更加常用的是softmax模型."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多请参考:\n",
    "\n",
    "[1] 李航统计学方法$P_{223}$\n",
    "\n",
    "[2] [Logistic回归(Logistic Regression)算法](https://www.jianshu.com/p/bbdeb356057e)\n",
    "\n",
    "[3] [浅析Logistic Regression](picture/LRegression.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
