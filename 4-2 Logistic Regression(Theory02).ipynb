{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化不单单只是在LR中出现,它是一种通用的思想,主要解决模型存在的过拟合,欠拟合的情况都可以使用正则化来降低两者的发生.\n",
    "\n",
    "其实监督的机器学习问题无非就是\"minimize your error while regularizing your parameters.\",也就是规则化模型参数的同时最小化误差.\n",
    "\n",
    "最小化误差是为了让我们的模型拟合我们的数据,规则化参数是为了防止我们的模型过分拟合,其中最小化误差我们已经在Theory01中解决了,现在就剩下规则化参数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Over-fitting and Under-fitting\n",
    "\n",
    "#### 1.1 Over-fitting\n",
    "\n",
    "比如下图就存在过拟合的现象:\n",
    "\n",
    "<img src=\"picture/60.png\" width=400 heigth=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说寻找到的最优参数太过于注重训练样本了,导致最终的分类边界(超平面)会如上图所示,那么这样的结果会使得对测试样本拟合结果的错误率非常高,通常体现为:\n",
    "\n",
    "训练样本正确率非常高,而测试样本正确率很低.\n",
    "\n",
    "用人话说就是:你太注重一个树,而忽略了整片森林的美好."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Under-fitting\n",
    "\n",
    "<img src=\"picture/61.png\" width=400 heigth=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么这张图就是欠拟合的现象,模型的拟合参数无法支撑整个数据的正确率,波动很大.\n",
    "\n",
    "体现为:\n",
    "\n",
    "偶尔训练样本或者测试样本正确率高,偶尔底.\n",
    "\n",
    "用人话说就是:你太浪."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正是由于过拟合和欠拟合的存在,我们需要使用规则化来解决这两个问题.使得你变成一个潇洒的人.\n",
    "\n",
    "也就是说通过规则化之后,我们希望看到的是下图的情况:\n",
    "\n",
    "<img src=\"picture/62.png\" width=400 heigth=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 规则化函数$Ω(w)$也有很多种选择,一般是模型复杂度的单调递增函数,模型越复杂,规则化值就越大.比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同,取得的效果也不同,但我们在论文中常见的都聚集在:零范数,一范数,二范数,迹范数,Frobenius范数和核范数等等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们就来看看目前需要的几个范数剩余的一些将在之后慢慢道来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 规则化:\n",
    "\n",
    "在说明规则化之前,我们要先了解一下说明叫稀疏化.实际上规则化的目的就是为了将参数稀疏.\n",
    "\n",
    "#### 2.1 稀疏化\n",
    "\n",
    "举个例子,比如目的要判定一个人患病的概率,那么这种病从收集到的数据而言假设有1000种特征(也就是说数据有1000个维度),那么很明显这种病的主要因素肯定不全是这1000种特征,假设通过学习之后得到的最优参数**$w^{*}$**只有很少的非零元素,比如是100个,那么我们就有理由相信影响这种病的主要因素就是这100个,那么对于新来的数据(同样维度也是1000),我们只要判定这1000个特征中的其中100个,我们就知道该人患这种病的几率.\n",
    "\n",
    "那么这里的最优参数$w^{*}$中很多元素是0,就表示稀疏.\n",
    "\n",
    "也就是说,对于一个样本$x_i$大部分特征都是对最终输出的预测值$\\hat{y}$没有任何提供信息,只有少部分特征提供最重要的信息.所以规则化的目的就是将参数稀疏化,但也不过于太稀疏.这里的**稀疏化**表示防止过拟合,**也不过于太稀疏**,表示防止欠拟合."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 范数的定义\n",
    "\n",
    "范数的一般化定义为:设$p\\geqslant 1$的实数,$P-norm$定义为:\n",
    "\n",
    "$||x||_p :=(\\sum_{i=1}^{n}|x_i|^{p})^{frac{1}{p}}$\n",
    "\n",
    "当$p=1$:为L1范数.\n",
    "\n",
    "$||x||_1 :=\\sum_{i=1}^{n}|x_i|^{1}$\n",
    "\n",
    "当$p=2$:为L2范数.\n",
    "\n",
    "$||x||_2 :=(\\sum_{i=1}^{n}|x_i|^{2})^{\\frac{1}{2}}$\n",
    "\n",
    "当$p=0$:为L0范数.\n",
    "\n",
    "实际上当$p=0$的时候,已经不能称之为范数了,因为其不满足三角不等性.但是仍然有很多人称之为L0范数,因为其有重要的理论作用.\n",
    "\n",
    "$||x||_0 :=(\\sum_{i=1}^{n}|x_i|^{0})^{\\frac{1}{0}}$\n",
    "\n",
    "很明显,这个式子是无法理解的,所以我们将给出下面的代替:\n",
    "\n",
    "$||x||_0= \\sum_{i=1}^{n}(x_i)\\;with\\;x_i \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其几何形状为:\n",
    "\n",
    "![](picture/64.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这三个范数有很多非常有意思的特征,尤其是在机器学习中的正则化(Regularization)以及稀疏编码(Sparse Coding)有非常有趣的应用.\n",
    "\n",
    "#### 2.3 不同正则化分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么有了上诉的三个范数,我们来定义监督学习的最优参数正则化公式:\n",
    "\n",
    "$w^{*}=\\underset{w}{argmin}loss(w)+\\lambda \\Omega(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "- 这里的$\\Omega(w)$就表示不同的正则化项\n",
    "- 第一项损失函数:\n",
    "    - Square loss:最小二乘\n",
    "    - Hinge Loss:那就是大名鼎鼎的SVM(后面会讨论)\n",
    "    - exp Loss:就是Boosting(后面会讨论)\n",
    "    - log Loss: Logistics Regression\n",
    "- 这里的$\\lambda$又称为惩罚项,是需要调节的一个参数,惩罚项大于0,不可过大.\n",
    "\n",
    "- 加入$\\lambda$是为了控制模型是否使用正则化,当$\\lambda =0$的时候就是不使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么这三个范数能稀疏化$w$呢?\n",
    "\n",
    "\n",
    "$L_0$范数:向量中非0元素的个数,通过最小化$w$中非零的个数来使得$w$变得稀疏从而找到最优的特征,但是最小化$w$中非零的个数会产生[NP](https://zh.wikipedia.org/wiki/NP_(%E8%A4%87%E9%9B%9C%E5%BA%A6))问题.所以我们会使用更高维的范数$L_1,L_2$.\n",
    "\n",
    "$L_1$范数(LASSO regularizer):是指向量中各个元素绝对值之和\n",
    "\n",
    "针对于LR解释一下(其实其他的损失函数都是类似的)\n",
    "\n",
    "LR loss(single sample):\n",
    "\n",
    "### $loss(w,b) = \\frac{-[y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i}] + \\lambda {||w_i||}_1)}{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$loss(w,b)$求导和之前的一样使用链式法则,只是最后会有个$\\lambda sgn(w_i)$.\n",
    "\n",
    "所以最后的参数更新就变为了:\n",
    "\n",
    "$w := w-\\alpha(dw+\\frac{\\lambda}{m} sgn(w_i))=w-\\alpha \\frac{\\lambda}{m} sgn(w_i) -\\alpha dw$\n",
    "\n",
    "\n",
    "式子中$\\alpha dw$我们无需理会,这是Theory01的内容,与我们现在的分析无关.\n",
    "\n",
    "那么实际上$w$的是否会稀疏只与$w- \\alpha \\frac{\\lambda}{m} sgn(w_i)$有关.\n",
    "\n",
    "由于对于$w_i$的求导可能产生两种结果(这里不考虑$w_i=0$的情况,实际上$w_i$很少很少会是0):\n",
    "\n",
    "(1) 如果$w_i>0$,那么得到的导数是1,所以在更新$w_i$的时候$w_i$会减去一个正数$\\lambda$,从而$w_i$更加接近于0.\n",
    "\n",
    "(2) 如果$w_i<0$,那么得到的导数是-1,所以在更新$w_i$的时候$w_i$会减去一个负数$\\lambda$,从而$w_i$更加接近于0.\n",
    "\n",
    "这样就实现了$w_i$的稀疏化\n",
    "\n",
    "**Ps:**\n",
    "- 因为这里$b$项不在$w$中,所以我们无需理会,对于我们现分析没有什么影响.\n",
    "- $sgn$表示:当$w_i>0$的时候为1,当$w_i<0$的时候为-1.\n",
    "\n",
    "- 对于$L_1$范数更规范的说法是:\n",
    "    - 它是L0范数的最优凸近似\n",
    "    - 任何的规则化算子,如果他在$W_i=0$的地方不可微(也就是$w_i\\neq 0$),并且可以分解为一个\"求和\"的形式,那么这个规则化算子就可以实现稀疏.\n",
    "\n",
    "- $m$:为样本数量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_2$范数(Ridge Regularizer,weight decay):是指向量中各个元素平方之和然后再开方\n",
    "\n",
    "LR loss(single sample):\n",
    "\n",
    "这里我们为了处理方便将$L_2$范数取平方(平方并不更改几何形状)即:$||w_i||^{2}_{2}=(||w_i||_{2}^\\frac{1}{2})^2$\n",
    "\n",
    "### $loss(w,b) = \\frac{-[y_ilog(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i})]}{m} + \\frac{\\lambda}{2m} {||w_i||}^{2}_2$\n",
    "\n",
    "同样求导且在$w_i=0$处不可微:\n",
    "\n",
    "更新参数的形式为:\n",
    "\n",
    "$w := w-\\alpha(dw+\\frac{\\lambda}{m} \\cdot w_i)=w-\\alpha \\frac{\\lambda}{m} \\cdot w_i -\\alpha dw$\n",
    "\n",
    "\n",
    "同样考虑:\n",
    "\n",
    "(1) 如果$w_i>0$,更新$w_i$的时候$w_i$会减去一个正数$\\lambda \\cdot w_i$,从而$w_i$更加接趋近于0.\n",
    "\n",
    "(2) 如果$w_i<0$,更新$w_i$的时候$w_i$会减去一个负数$\\lambda \\cdot w_i$,从而$w_i$更加接趋近于0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "- 这里的损失函数正则项除上2是为了求导数的时候将实数约掉,因为实数对参数更新的影响不大."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上$L_2$范数会比$L_1$范数更加实用,因为$L_2$只是做衰减(decay),只会将$w_i$尽量向0靠近,减少权重,而$L_1$比较猛,直接就是向0使劲靠.\n",
    "\n",
    "用人话说:$L_2$比较温和,$L_1$比较生猛,俗话说的好,废材也有重要的地方,所以我们不能完全丢弃废材."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 图像化解释$L_2$优于$L_1$\n",
    "\n",
    "<img src=\"picture/65.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在图中,左边是$L_1$,右边是$L_2$,可以看出:\n",
    "\n",
    "对于$L_1$因为有菱角的时候,所以只有目标函数摆的非常好的时候.参数稀疏的可能才会大(也就是两个图的交点)\n",
    "\n",
    "\n",
    "而$L_2$没有菱角性质,只要有相交的地方即可稀疏参数,所以$L_2$比$L_1$更加实用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实到这里所有的正则优化就已经完成了,不过我们还可以在不同的角度对正则化进行说明:\n",
    "\n",
    "#### 3 正则化的不同角度\n",
    "\n",
    "(1) 奥卡姆剃刀(Occam's razor)原理:在所有可能选择的模型中,我们应该选择能够很好地解释已知数据并且十分简单的模型.\n",
    "\n",
    "(2) 从贝叶斯估计的角度来看，规则化项对应于模型的先验概率\n",
    "\n",
    "- 根据[最大熵原理](https://wanghuaishi.wordpress.com/2017/02/21/%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%88the-maximum-entropy-principle%EF%BC%89/),我们知道的信息越多,越对我们预测的结果有好处,那么实际上就是一个先验的想法而增加正则项实际上就是增加一个先验概率,依照先验概率再去预测结果,这样因为我们知道的\"信息\"更多,所以预测的也就越准确\n",
    "- 这个最大熵理论是很厉害,感兴趣建议一定要仔细看看.\n",
    "\n",
    "(3) 民间说法:规则化是结构风险最小化策略的实现,是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
