{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上Softmax多配合于神经网络,在这里提出只是为了与LR相对应.\n",
    "\n",
    "### 1 Softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 可以用多分类问题,而不像LR等二类模型那样需要借助[One-vs-rest](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB)推广到多分类.\n",
    "\n",
    "设样本$x_i$的真实标签类别$y_i\\in \\{1,2,...,C\\}$.同样我们需要借助One-Hot向量$y_i=(0,0,...,1,...0)$只有对应真实类别的那一维是1,长度为C.参照Naive Bayes(Application)文档中的例子.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "- 多分类: 是将示例归类为多个(大于两个)类别中的一类(将示例归为两类中的一类被称为二元分类).\n",
    "- 多标签: 多标签分类要为每个示例预测多个标签,即一个示例可以同时被归为多个类别,比如一部电影可以有很多标签\"爱情,动作,喜剧\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Softmax 后验估计\n",
    "\n",
    "$z_c = w_cx+b$\n",
    "\n",
    "$softmax=\\sigma{(z_c)}=\\frac{exp(z_c)}{\\sum_{c=1}^{C}exp(z_c) }$\n",
    "\n",
    "与LR类似,softmax输出的是样本$x_i$属于各个类别的后验概率估计值,对于给定数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\},y_i\\in\\{0,1,...,C\\}$\n",
    "\n",
    "将每一个$y_i$转换成one-hot形式(对应类别的值为1,其余为0):\n",
    "\n",
    "$Y=\\begin{bmatrix}\n",
    "1 &0  &...  &0 \\\\ \n",
    " 0&1  &...  &0 \\\\ \n",
    "... & ... & ... &... \\\\ \n",
    " 0&0  &...  & 1\n",
    "\\end{bmatrix}_{C,N}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么:\n",
    "\n",
    "$P(Y=0|X)=(1,0,0...0)_{C}$\n",
    "\n",
    "$P(Y=1|X)=(0,1,0...0)_{C}$\n",
    "\n",
    "...\n",
    "\n",
    "$P(Y=C|X)=(0,0,0...C)_{C}$\n",
    "\n",
    "所以似然函数为:\n",
    "\n",
    "$\\prod P(Y=c|X)= \\prod [\\hat{y_i}]^{y_i}$\n",
    "\n",
    "对数形式:\n",
    "\n",
    "$logP(Y=c|X)=\\sum_{i=1}^{N}y_ilog\\hat{y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么其损失函数(经验风险)为:\n",
    "\n",
    "$loss(w,b)= -\\frac{1}{N}\\cdot \\sum_{i=1}^{N}y_ilog\\hat{y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "\n",
    "这里的似然函数也可以直接由LR的似然函数直接看出\n",
    "\n",
    "$\\prod_{i=1}^{N}[\\hat{y_i}]^{y_i}\\cdot[1-\\hat{y_i}]^{1-y_i}$,相应的标签为1,其余的全为0,所以相应标签1带入似然函数,就得以得到上述形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Parameters\n",
    "\n",
    "假设现在我们的标签有4类,那么:\n",
    "\n",
    "$a=\\sigma(z)=\\frac{exp(z_i)}{exp(z_1)+exp(z_2)+exp(z_3)+exp(z_4)}$\n",
    "\n",
    "很明显,我们对其中一个$z_i$进行求导的话,会发现分母部分会有与$i$项无关的情况,所以我们将$a$对$z$求导分为两个部分:\n",
    "\n",
    "(1) 当$i=j$:\n",
    "\n",
    "$\\frac{\\partial a_i}{\\partial z_i} = \\frac{e^{z_i}\\sum_j e^{z_j}-(e^{z_i})^2}{(\\sum_j e^{z_j})^2} = \\frac{e^{z_i}}{\\sum_j e^{z_j}}-\\frac{(e^{z_i})^2}{(\\sum_j e^{z_j})^2} = a_i - a_i^2 = a_i(1-a_i)$分子可以求导，分母除了i项其他都为常数求导为0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 当$i\\neq j$:\n",
    "\n",
    "$\\frac{\\partial a_i}{\\partial z_j} =\\frac{0\\cdot \\sum_j e^{z_j} - e^{z_j}e^{z_i}}{(\\sum_j e^{z_j})^2} = -a_ia_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么我们将两部分组合求损失函数对Z求导部分(链式法则):\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z}= \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial z}$ \n",
    "\n",
    "$= -\\sum_j \\frac{y_j}{a_j}\\cdot \\frac{\\partial a_j}{\\partial z_j}-\\sum_i \\frac{y_i}{a_i}\\cdot \\frac{\\partial a_i}{\\partial z_i}$\n",
    "\n",
    "$=-\\sum_i \\frac{y_i}{a_i}a_i(1-a_i) + \\sum_{i\\neq j}\\frac{y_j}{a_j}a_ia_j = -\\sum_iy_i + \\sum_iy_ia_i+a_i\\sum_{i \\neq j} y_j$\n",
    "\n",
    "$ = -\\sum_i y_i + a_i \\sum y$ \n",
    "\n",
    "\n",
    "因为针对分类问题y的值只可能是一个1,其他都是0,所以结果为$\\sum y = 1$\n",
    "\n",
    "所以最终的结果是:$a_i - y_i$\n",
    "\n",
    "<P style=\"color:orange\">可以看出其和LR的结果是一样的.</p>\n",
    "\n",
    "所以:\n",
    "\n",
    "$dw_i=\\frac{\\partial loss}{\\partial w_i}=x(a_i-y_i)$\n",
    "\n",
    "$db_i=\\frac{\\partial loss}{\\partial b_i}=(a_i-y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 softmax Algorithm\n",
    "\n",
    "下面给出softmax的算法.\n",
    "\n",
    "**Softmax Algorithm:**\n",
    "\n",
    "输入:样本数据$X=\\{x_1,x_2,...,x_m\\}$,样本标签$Y=\\{y_1,y_2,...,y_m\\}$,其中每个样本$x_i$有n个特征.每个标签$y_i\\in \\{1,2,...,C\\}$,学习率$\\alpha$\n",
    "\n",
    "输出:样本预测标签$\\hat{Y}$\n",
    "\n",
    "(1) 将样本标签转换成\"Hot\"形式:$Y=\\begin{bmatrix}\n",
    "1 &0  &...  &0 \\\\ \n",
    " 0&1  &...  &0 \\\\ \n",
    "... & ... & ... &... \\\\ \n",
    " 0&0  &...  & 1\n",
    "\\end{bmatrix}_{C,N}$,即对应的类别为1,其余为0.初始化$W,b$\n",
    "\n",
    "(2) 计算线性值$Z=WX+b$\n",
    "\n",
    "(3) 计算预测值$A=softmax(Z)$\n",
    "\n",
    "(4) 计算损失函数$loss(w,b)= -\\frac{1}{m}\\cdot \\sum_{i=1}^{m}y_ilog\\hat{y_i}$\n",
    "\n",
    "(5) 更新参数:\n",
    "    \n",
    "$W{:=}W-\\alpha dW$\n",
    "    \n",
    "$b:=W =\\alpha db$\n",
    "\n",
    "(6) 在损失精度阈值$\\epsilon$内退出迭代,即$|loss^{k+1}-loss^{k}|<\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Example of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设单个样本的线性回归计算完后的$Z$如下所示:\n",
    "\n",
    "$Z = \\begin{bmatrix}\n",
    "5\\\\ \n",
    "2\\\\ \n",
    "-1\\\\\n",
    "3\\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中每一列代表一个分类(上述有4个分类).那么softmax如下所示:\n",
    "\n",
    "$t = \\begin{bmatrix}\n",
    "e^{5}\\\\ \n",
    "e^{2}\\\\ \n",
    "e^{-1}\\\\\n",
    "e^{3}\\\\ \\end{bmatrix} = \\begin{bmatrix}\n",
    "148.4\\\\ \n",
    "7.4\\\\ \n",
    "0.4\\\\\n",
    "20.1\\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a = \\frac{t}{176.3} = \\begin{bmatrix}\n",
    "0.842\\\\ \n",
    "0.042\\\\ \n",
    "0.002\\\\\n",
    "0.114\\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出实际上4个分类的求和为1,也就是代表着,softmax实际上是将一个融合概率拆分成4个子概率,其中0.842最大,说明这个样本属于第一类的概率最大."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
