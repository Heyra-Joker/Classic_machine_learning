{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Separating data with the maximum margin\n",
    "\n",
    "To introduce the subject of support vector machines I need to explain a few concepts. Consider the data in frames A–D in figure 6.1; could you draw a straight line to put all of the circles on one side and all of the squares on another side?\n",
    "\n",
    "Now consider the data in figure 6.2, frame A. There are two groups of data, and the data points are sep- arated enough that you could draw a straight line on the figure with all the points of one class on one side of the line and all the points of the other class on the other side of the line.we say the data is \"linearly separabel\".\n",
    "\n",
    "![](picture/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line used to separate the dataset is called a **separating hyperplane**. In our simple 2D plots, it’s just a line. But, if we have a dataset with three dimensions, we need a plane to separate the data; and if we have data with 1024 dimensions, we need something with 1023 dimensions to separate the data. What do you call something with 1023 dimensions? How about N-1 dimensions? It’s called a **hyperplane**. The hyperplane is our decision boundary. Everything on one side belongs to one class, and everything on the other side belongs to a different class.\n",
    "We’d like to make our classifier in such a way that the farther a data point is from the decision boundary, the more confident we are about the prediction we’ve made. Consider the plots in figure 6.2, frames B–D. They all separate the data, but which one does it best? Should we minimize the average distance to the separating hyperplane? In that case, are frames B and C any better than frame D in figure 6.2? Isn’t something like that done with best-fit lines? Yes, but it’s not the best idea here. We’d like to find the point closest to the separating hyperplane and make sure this is as far away from the separating line as possible. This is known as **margin**. We want to have the greatest possible margin, because if we made a mistake or trained our classifier on limited data, we’d want it to be as robust as possible.\n",
    "\n",
    "![](picture/13.png)\n",
    "\n",
    "The points closest to the separating hyperplane are known as **support vectors**. \n",
    "\n",
    "**就是找到分割超平面最近的那些点。接下来试着最大化支持向量到分隔面的距离**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.详解SVM原理\n",
    "\n",
    "**Note:**内容原文来自**数说工作室(微信公众号)**，强烈建议读者阅读数说工作室的[知乎原文SVM1-6](https://zhuanlan.zhihu.com/ishushuo)下面我将归纳整理下该文章的思想过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1 开篇:\n",
    "实际上支持向量机(SVM)也是线性分类器中的一个算法，但是它的牛逼之处在于对于\n",
    "- 小样本而言SVM可以进行精准打击。\n",
    "- 对于非线性划分的情况下，SVM可以使用“高维映射”的技术，但是由于高维会产生维度灾难，所以SVM会使用一种叫\"核函数\"的东西去在低纬度下进行运算从而避免维度灾难。如果不知道什么叫维度灾难和核函数没关系，接下去的部分会说明。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2 :\n",
    "\n",
    "#### part 2.1线性分类:\n",
    "线性分类器:在一个二维空间里，我们可以使用一个线性函数将样本点全部分开，那么称这些数据是线性可分的，否则称为线性不可分.这个线性函数在一维空间里就是一个点，在二维空间里是一条线，在三维空间里是一个平面，在四维空间里无法想象.. 不过它们都可以统称为\"超平面\".如图所示，在一个二维空间里，一条线很好的将圆圈和叉叉分开了，这条线就叫超平面:\n",
    "![](picture/15.png)\n",
    "\n",
    "如果我们使用数学公式表示这条直线，那么这条线应该为$f(x)=w_1x_1+w_2x_2+b=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么此时，有\n",
    "- 当某点带入 $f(x)$是的$f(x)>0$,则该点在直线上方，说明属于圆圈.\n",
    "- 当某点带入 $f(x)$是的$f(x)<0$,则该点在直线上方，说明属于叉叉.\n",
    "- 实际上$f(x)$还可能是0,也就是在这条直线上,那么这种情况的话，新进来的一个点到底属于圆圈还是叉叉就不知道了.实际上这个牵扯到的问题叫\"不适定问题\".实际上我们可以将直线稍微移动一下(当然依然是可以作为超平面),那么就会产生一条新的直线，那么到底是第一条直线好，还是第二条直线好呢?这就是一个不适定问题.![](picture/16.png)\n",
    "\n",
    "- 事实上,我们需要找一个超平面,能把两个类的点**最分开、分开的最清楚**的超平面才是最好的超平面应为它更加的健壮(也就是新数据进来很少落在超平面附近或者超平面上)\n",
    "那么上面的思想就是**朴素**的思想\n",
    "\n",
    "*在诉说下面的内容前，我们先来看看Logistic模型和SVM模型的区别*\n",
    "- 寻找最优平面的方法不同\n",
    "    - logistic模型找的超平面是尽量让所有的点都远离它,而SVM只是让超平面附近的点远离(这些点称之为**支持向量**，或者叫支持向量的样本)，这一点非常重要.\n",
    "- SVM可以处理非线性的情况\n",
    "    - 比logistic你牛逼的地方是SVM支持非线性的情况![](picture/17.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part 2.2 数学表达式:\n",
    "\n",
    "- 超平面的数学表达式:\n",
    "- 二维空间下:$f(x)=w_0x_0+w_1x_1+b$\n",
    "- 三维空间下:$f(x)=w_0x_0+w_1x_1+w_1x_1+b$\n",
    "- 我们可以扩展为更高维度下:\n",
    "    - $f(x)=w_0x_0+w_1x_1+\\cdots+w_nx_n+b=(w_0,\\cdots,w_n)\\begin{pmatrix} x_0\\\\ \\cdots\\\\ x_n\n",
    "\\end{pmatrix} + b = W^{T}X+b$\n",
    "\n",
    "因此我们直接使用向量的形式就可以表示任意维度下的超平面$f(x) = W^{T}X+b$\n",
    "\n",
    "那么根据我们的判断标准:\n",
    "- 当某点带入 $f(x)$是的$f(x)>0$,则该点在直线上方，说明属于圆圈.\n",
    "- 当某点带入 $f(x)$是的$f(x)<0$,则该点在直线上方，说明属于叉叉.\n",
    "\n",
    "量化表达为: 我们用y这个字母来代表某点所属的类别,也叫标签比如y=1代表圆圈,y=-1代表叉叉,实际上在logistic中我们经常会使用(0,1)这种形式而不是(1,-1)这种形式，那么我们为什么会使用(1,-1)这种形式呢?实际上是与SVM的计算方式有关(下面会说到),只是设置(1,-1)我们能够更加方便的计算,无论如何，最终的判断形式如下:\n",
    "    \n",
    "$y=\\left\\{\\begin{matrix}\n",
    "1,&f(x)\\geq 0 \\\\\n",
    " -1&f(x)<  0 \\\\\n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 3:\n",
    "#### 1 函数间隔:\n",
    "\n",
    "一个直观的想法，将某个点带入超平面$f(x)$中 $|f(x)=W^{T}X+b|$ 越大说明这个点离这条直线越远(也就是说如果 $f(x)>>0$,那么就可以将该点归类于圆圈,如果$f(x)<<0$,那么将该点归类于叉叉,如果不幸等于$f(x)==0$,那么就正好在超平面上),反过来说也就是这个超平面分割的功能越好.那么这个$|f(x)|$就称之为\"**间隔函数**\",但是我们不会使用$|f(x)|$,因为绝对值这个东西是真的很不友好.那么我们会使用$yf(x)$,那么上面说的$y=1,-1$的好处就可以体现了，因为如果$f(x)<0$,那么y=-1整个$yf(x)>0$就类似于$|f(x)|$的值,反过来也是一样的在$f(x)<<0$的时候.\n",
    "\n",
    "那么我们重新定义函数间隔就是:$yf(x) = \\tilde{r}$\n",
    "\n",
    "例子:如果$yf(X1)>yf(X2)$,那么X1的点距离超平面的距离一定大于X2的点距离超平面的距离![](picture/18.png)\n",
    "\n",
    "如果样本数量非常多,那么我们是计算所有样本点和超平面的距离呢还是只计算两类样本中分别距离超平面最近的点到超平面的距离呢?\n",
    "\n",
    "答案很显然是只计算两类样本中分别距离超平面距离最近的点到超平面的距离,也就是上面说到的**支持向量**.\n",
    "\n",
    "就是下面红色的圆点和绿色的叉叉\n",
    "\n",
    "![](picture/19.png)\n",
    "\n",
    "\n",
    "**实际上到这里我们就可以先说说SVM到底要做的是什么了**\n",
    "- 设立超平面,由于超平面具有不适定性,所以我们要找到最优的超平面\n",
    "- 为了找到最优的超平面我们要先找到距离超平面最近的样本点也就是支持向量\n",
    "- 然后让这些支持向量尽可能的远离超平面,这样的分类器才是健壮的,才能最好的将样本分开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么现在还有一个问题就是找**支持向量**使用的函数间隔这种形式真的合适吗？实际上是不合适的，因为牵扯到一个东西叫做同比例缩放(下面会说),所以我们需要使用更好的方式**几何间隔**来找出这些支持向量(也就是距离超平面最近的点)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.几何间隔\n",
    "\n",
    "实际上在二维上超平面是一条线，计算点到这条线的距离就是点到直线的距离:\n",
    "\n",
    "# $d = \\frac{|w_0x_0+w_1x_1+b|}{\\sqrt{w_0^2+ w_1^2}}$\n",
    "\n",
    "但是如果是在高维情况下,那么某点到直线的几何距离就变成了:\n",
    "\n",
    "# $d = \\frac{\\tilde{r}}{||W^{T}||}$\n",
    "\n",
    "其中 \n",
    "- $||W^{T}|| = \\sqrt[p]{w_{1}^{p}+w_{2}^{p} + \\cdots +w_{n}^{p}}$,称之为向量$W^{T}=(w_1,w_2,..,w_n)$的范式\n",
    "- $\\tilde{r}=yf(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刚刚说的几何间隔比函数间隔要好是因为,当同比例缩放$f(x)$中的系数$w,b$时,那么直线还是那条直线,但是函数间隔会变，但是几何间隔不会,因为我们会除上$W^{T}$的范式."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么现在我们将SVM要做什么更改成数学的语言\n",
    "#### 3.最大间隔分类器\n",
    "- 由于超平面具有不适定性,所以为了寻找到最好的超平面,我们需要使用几何距离来衡量点到超平面之间的距离,目的是为了找到支持向量\n",
    "- 一组向本中,将点到超平面距离最近的点的距离,作为样本到超平面的距\n",
    "- 最大化这个距,才能最好的将样本分开\n",
    "\n",
    "![](picture/20.png)\n",
    "\n",
    "将上面三句话转换成数学语言就是:\n",
    "\n",
    "### $r =  \\frac{\\tilde{r}}{||W^{T}||} =  \\frac{|f(x)|}{||W^{T}||} =  \\frac{yf(x)}{||W^{T}||}$\n",
    "### $r=min r_i(i=1,2,..,n) 也即y_i(W^{T}x_i+b)\\geqslant \\tilde{r}$\n",
    "\n",
    "$\\tilde{r}$ 可以考虑成支持向量,也就是距离超平面最短的点,那么其他的点距离这个超平面的距离一定要大于等于这个$\\tilde{r}$\n",
    "\n",
    "### $max r=max\\frac{\\tilde{r}}{||W^{T}||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里为了方便推导,我们将固定函数间隔为$|f(x)|=yf(x)=1$,实际上固定这个值对函数的优化没有任何影响.原因如下(下面的解释来自CSDN博客作者:*v_JULY_v的阐述*):\n",
    "\n",
    "1.对于任意一个线性可分问题,任意给定一个分类正确的超平面,都有一个最小函数间隔,为了方便书写记做r^,这个r^是方向向量w和截距b的函数，因为分类点是给定的\n",
    "\n",
    "2.也就是说r~=r^(w,b)，求最大几何间距的问题就是求出一组数据w，b使得r^/||W^{T}||最大,此约束条件是**任意给定的一个点的函数间隔大于等于r^**,也就是$y_i(W^{T}x_i+b)\\geqslant$r^.\n",
    "\n",
    "3.下面做一个变量替换，用w' = w/r^和b'=b'/r^代替上面的w和b,这样的新变量人仍然是w和b的函数所以最大化仍然是可以进行的。于是把两个新变量带进到原约束条件中,那么约束条件就变成了$y_i(W^{T}x_i+b)\\geqslant 1$.这样一来就通过变换得到一个本来关于w和b的问题变成了w'和b'的问题.进而也就变成了$\\frac{1}{||W^{T}||}的问题$\n",
    "\n",
    "\n",
    "4.实际上这样变换只是为了说明方便,实际上可以直接说函数间隔直接缩放(函数同比例缩放),缩放大小为1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么 $max\\frac{1}{||W^{T}||}$ ,求$max\\frac{1}{||W^{T}||}$的最大值相当于求$\\frac{1}{||W^{T}||}的最小值$,也就等价于$\\frac{1}{2}||W^{T}||^{2}$，这里的$\\frac{1}{2}和^2$只是为了计算方便，并不改变$||W||$的取值方向.\n",
    "\n",
    "那么最终问题等价于:\n",
    "## $min \\frac{1}{2}||W^{T}||^2$\n",
    "s.t. $y_i(W^{T}x_i+b)\\geqslant 1,i=1,2..,n$\n",
    "\n",
    "\n",
    "下面就是如何求解这个最小值了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "#### 1.不等式拉格朗日乘数法\n",
    "---------------------\n",
    "\n",
    "我们先来回顾一下什么叫拉格朗日乘数法\n",
    "\n",
    "假设我们要求f(x)的最小值,约束条件是h(x) =0，即\n",
    "\n",
    "Min f(x)\n",
    "\n",
    "s.t h_i(x) = 0,i=1,2,...n\n",
    "\n",
    "那么可以引入拉格朗日算子a,构造拉格朗日函数:\n",
    "\n",
    "$F(x,a) = f(x)-\\sum_{i=1}^{n}a\\cdot h_i(x)$\n",
    "\n",
    "然后对x和a求偏导,使得偏导数等于0然后解出x,a.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我们这里遇到的不是一个等式约束,而是一个不等式约束.\n",
    "\n",
    "我们仍然可以通过拉格朗日函数将条件融合到目标函数中:\n",
    "\n",
    "**原函数:**\n",
    "### $min \\frac{1}{2}||W^{T}||^2$\n",
    "s.t. $y_i(W^{T}x_i+b)\\geqslant 1,i=1,2..,n$\n",
    "\n",
    "**构造函数:**\n",
    "\n",
    "### $F(w,b,a)=\\frac{1}{2}||W^T||^2-\\sum_{i=1}^{n}a_i\\cdot [y_i(W^{T}x_i+b)-1]$\n",
    "\n",
    "**令:**\n",
    "\n",
    "### $C(W)=\\underset{a_i>0}{max}F(w,b,a)$\n",
    "\n",
    "这个函数意味着通过调整a的大小可以使得F函数最大(因为要运算的是$\\frac{1}{2}||W^{T}||^2$).\n",
    "- 当约束条件满足时,我们最大的F函数能提取到$\\frac{1}{2}||W^T||^2$.也就是有$C(W)=\\frac{1}{2}||W^T||^2$\n",
    "- 当约束条件不满足时,即y_i(W^{T}x_i+b)< 1,一定可以调整a使得c(w)无穷大，这很简单就是令a为正无穷.\n",
    "\n",
    "**最后原命题就等价于:**\n",
    "\n",
    "### $\\underset{w,b}{min}C(w)=\\underset{w,b}{min} \\underset{a_i>0}{max}F(w,b,a)=P^{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.拉格朗日对偶问题\n",
    "\n",
    "上面我们把不等于约束问题转换成了一个关于$p^{*}$的问题。\n",
    "\n",
    "$\\underset{w,b}{min}C(w)=\\underset{w,b}{min} \\underset{a_i>0}{max}F(w,b,a)=P^{*}$\n",
    "\n",
    "实际上这个问题还是很难解决,因为我们要解决不等式约束的max问题,然后再在w,b上求最小值.回到我们最初的问题:求最大几何间距的问题就是求出一组数据最优的w,b。所以我们可以先求w,b再求最大的a_i是一个不错的选择.即\n",
    "\n",
    "$ \\underset{a_i>0}{max}\\underset{w,b}{min}F(w,b,a)=d^{*}$\n",
    "\n",
    "那么$d^{*}$就是$p^{*}$的对偶形式,但是这两者未必相等,实际上他们的关系是\n",
    "\n",
    "$d^* \\leq P^*$\n",
    "\n",
    "这个叫做\"弱对偶性质(week duality)\",意思是最大值中的最小值也要比最小值中的最大值要大.以常识的理解也是可以理解的(也就是$P^{*}min >= d^{*}max$).同时也会产生一个**对偶间隙**$P^{*}-d^{*}$\n",
    "\n",
    "那么我们只要消除这个对偶间隙，那么上述的两种形式就相等了,即\n",
    "\n",
    "$\\underset{w,b}{min}C(w)=\\underset{w,b}{min} \\underset{a_i>0}{max}F(w,b,a)=P^{*}=\\underset{a_i>0}{max}\\underset{w,b}{min}F(w,b,a)=d^{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是要消除这个对偶间隙不那么简单，需要牵扯到两个定理(Slate和KKT,看不懂就忽略,其实对于推理不重要):\n",
    "\n",
    "- Slate(凸优化理论中): \n",
    "    - 凸优化问题存在严格可行解,即存在x,，满足\n",
    "    - $f_i(x)<=0,i=1,2,...n$\n",
    "    - $Ax=b$\n",
    "    - 则优化问题具有强对偶性,也就是说上述$d^{*},P^{*}$的对偶间隙就会消除,即$d^{*}=P^{*}$\n",
    "    - 简而言之就是说存在一个x,使得不等式约束中的\"小于等于号\"要严格取到\"小于号\".\n",
    "    \n",
    "但是光是Slate条件成立是不行的，因为$d^{*},P^{*}$可能不止一个解，我们要保证$d^{*}=P^{*}$为最优解的时候才可以,那么就要使用到\"KKT\"条件,\n",
    "个人认为这个KKT条件是真的恶心.\n",
    "\n",
    "- KKT(F(w,b,a)对w,b都可微):\n",
    "    - 如果对应的拉格朗日函数为:$L(x,\\mu_1\\mu_2)=f(x)+\\mu_1g_1(x)\\mu_2g_2(x)$\n",
    "    - 则KKT条件(具有局部最优点的必要条件)是:\n",
    "        - 1.$\\mu_1\\geqslant 0,\\mu_2\\geqslant 0$\n",
    "        - 2.$\\triangledown f(x)+\\mu_1\\triangledown g_1(x)+\\mu_2\\triangledown g_2(x)=0$,$\\triangledown $称作梯度\n",
    "        - 3.$\\mu_1g(x)+\\mu_2g(x)=0$\n",
    "\n",
    "只要满足上面两个条件,$P^{*}=d^{*}$的解即为最优条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面只要研究$d^{*}$就可以了\n",
    "\n",
    "### $F(w,b,a)=\\frac{1}{2}||W^T||^2-\\sum_{i=1}^{n}a_i\\cdot [y_i(W^{T}x_i+b)-1]=d^{*}$\n",
    "固定a,让F关于w和b最小化,对w,b求偏导数:\n",
    "\n",
    "### $\\frac{\\partial F}{\\partial w}=0\\Rightarrow w=\\sum_{i=1}^{n}a_iy_ix_i$\n",
    "- 这里就体现出了之前设置$\\frac{1}{2}||W^T||^2$的好处\n",
    "\n",
    "### $\\frac{\\partial F}{\\partial b}=0\\Rightarrow \\sum_{i=1}^{n}a_iy_i=0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后带入到F(w,b,a)中去\n",
    "\n",
    "$F(w,b,a)=\\frac{1}{2}||W^T||^2-\\sum_{i=1}^{n}a_i\\cdot [y_i(W^{T}x_i+b)-1]=d^{*}$\n",
    "    \n",
    "= $\\frac{1}{2}W^{T}W-W^{T}\\sum_{i=1}^{n}a_iy_ix_i-b\\sum_{i=1}^{n}a_iy_i+\\sum_{i=1}^{n}a_i$\n",
    "\n",
    "= $\\frac{1}{2}W^{T}\\sum_{i=1}^{n}a_iy_ix_i-W^{T}\\sum_{i=1}^{n}a_iy_ix_i+\\sum_{i=1}^{n}a_i$\n",
    "\n",
    "= $-\\frac{1}{2}W^{T}+\\sum_{i=1}^{n}a_i$\n",
    "\n",
    "= $-\\frac{1}{2}(\\sum_{i=1}^{n}a_iy_ix_i)^{T}+\\sum_{i=1}^{n}a_i$\n",
    "\n",
    "=$\\sum_{i=1}^{n}a_i-\\frac{1}{2}\\sum_{i,j=1}^{n}a_ia_jy_iy_jx_i^{T}x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们就可以对a求极大值了，也就是现在的问题就是剩下\n",
    "\n",
    "$\\underset{a}{max}(\\sum_{i=1}^{n}a_i-\\frac{1}{2}\\sum_{i,j=1}^{n}a_ia_jy_iy_jx_i^{T}x_j)$\n",
    "\n",
    "s.t. $a_i\\geqslant0,i=1,2..n$, $\\sum_{i=1}^{n}a_iy_i=0$\n",
    "\n",
    "求出这个a的值就可以知道w和b的值了\n",
    "\n",
    "下面讲述**SMO高效优化算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "#### 在讨论SMO之前我们要先讨论一个牛逼的东西叫做核函数和什么是松弛变量\n",
    "\n",
    "但是在讨论核函数之前先来个小插曲,看看低纬度向高纬度转换,看看会产生什么问题,然后讨论如何解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们假设已经通过SOM高效优化算法得到了最优的$a_i$,那么我们知道:\n",
    "\n",
    "$W=\\sum_{i=1}^{n}a_iy_ix_i$\n",
    "\n",
    "那么这条线性分类器也就出来了,它是:\n",
    "\n",
    "$f(x)=W^{T}X+b=(\\sum_{i=1}^{n}a_iy_ix_i)^{T}X+b=\\sum_{i=1}^{n}a_iy_i<X_i,X>+b$\n",
    "\n",
    "式子中的<,>是内积.(注意这里只是假设使用了SOM,后面我们会讲)\n",
    "\n",
    "无论如何我们现在假设已经得到了最后的线性分类器，我们看的出来实际上在预测新的X点时,只需要计算它与训练数据点的内积就可以了\n",
    "\n",
    "且使用到的训练数据点也只是那些\"支持向量\",**即只有\"支持向量\"的点会被用来进行新样本的预测.**\n",
    "\n",
    "原因如下(我们从拉格朗日函数入手解释):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于w,b已经定下来了(也就是说不考虑$\\underset{w,b}{min}$),所以我们的拉格朗日函数的形式为:\n",
    "\n",
    " $\\underset{a_i\\geqslant 0}{max}F(w,b,a)=\\frac{1}{2}||W^T||^2-\\sum_{i=1}^{n}a_i\\cdot [y_i(W^{T}x_i+b)-1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果$x_i$是支持向量,那么根据我们之前定义的几何函数间隔为1(也就是支持向量与超平面的几何函数间隔),$y_i(W^{T}x_i+b)=1$,$a_i$是一个非0的数,故支持向量的点被纳入模型中(也就是说$<X_i,X>$会进行运算),进行新点的分类预测\n",
    "\n",
    "- 如果$x_i$是不支持向量那么$y_i(W^{T}x_i+b)>1$,这是显然的,因为其他非支持向量点到超平面的距离都要比支持向量点到超平面的距离大.那么在预测新点x时,由于$a_i$非负，为了满足最大化,$a_i$必须是0,那么在模型$f(x)=\\sum_{i=1}^{n}a_iy_i<X_i,X>+b$中由于$a_i$为0,所以后面部分为0,不纳入计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.非线性问题如何预测\n",
    "\n",
    "我们来看看下面这种情况,\n",
    "\n",
    "![](picture/22.png)\n",
    "\n",
    "像这种线性不可分(我们无法通过一条直线将两种类别分开)的情况我们该如何处理?\n",
    "\n",
    "事实上我们可以用曲线进行划分,比如一个二次函数\n",
    "\n",
    "![](picture/23.png)\n",
    "\n",
    "在二次函数的上方为一类,在二次函数的下方为一类,显然这样已经解决了一条直线无法划分两类的问题.\n",
    "\n",
    "那么我们用的这个曲线的数学形式的超平面就应该是\n",
    "\n",
    "$f(x)=w_0x^{2}+w_1x+w_3C+b = W^{T}\\begin{bmatrix}x^{2}\\\\ x\\\\ c\\end{bmatrix}+b=W^{T}H(x)+b$\n",
    "\n",
    "很明显这样的话我们将原来的一维x映射到了三维$(x^2,x,C)$上,也就能顺利的解决这种线性不可分的问题了,\n",
    "\n",
    "那么我们的预测函数就应该变成:\n",
    "\n",
    "$f(x)=W^{T}X+b=(\\sum_{i=1}^{n}a_iy_ix_i)^{T}X+b=\\sum_{i=1}^{n}a_iy_i<H(X_i),H(X)>+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么像这样我们就解决了再一维情况下不可分,我们可以将数据投射向高维情况进行划分,但是这样在维度不高的情况下是可行的,但是一旦特征很多,就会产生一个东西叫做**维数灾难**.\n",
    "\n",
    "我们来看看什么叫维数灾难:\n",
    "\n",
    "维数灾难（英语：curseof dimensionality，又名维度的诅咒）是一个最早由理查德·贝尔曼(Richard E. Bellman)在考虑动态优化问题时首次提出来的术语，用来描述当（数学）空间维度增加时，分析和组织高维空间（通常有成百上千维），因体积指数增加而遇到各种问题场景。这样的难题在低维空间中不会遇到，如物理空间通常只用三维来建模。\n",
    "\n",
    "数据的维数越高，会引发灾难，首先是计算量巨大，不用多说。其次，对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，分类器的性能不是得到改善，而是退化。因为在高维空间中，所有的数据都很稀疏，于是导致在相似度度量上，距离计算上都会出现很大的偏差，因为平时我们采用的算法也都会变得很低效。因此，多于多维数据，我们通常采用降维处理（[主成分分析](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)）\n",
    "\n",
    "正是由于维数灾难的存在,所以SVM使用一种技术叫做(核函数),我靠到这里才开始讲核函数,崩溃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.核函数\n",
    "\n",
    "核函数在计算的时候,它可以让x和y不用通过H()映射到高维空间再计算内积,而是直接在低维下计算\n",
    "\n",
    "我们用K()表示核函数,那么核函数的作用就是:\n",
    "\n",
    "$K(x,y)=<H(x),H(y)>$,从而避开了x映射到H(x)和y映射到H(y)的过程\n",
    "\n",
    "我们来举个例子:形式如以下\n",
    "$X=\\begin{pmatrix}x_1\\\\ x_2\\end{pmatrix}$,那么$H(x)=\\begin{pmatrix}x^2\\\\ x\\\\ c\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们c取值$\\frac{1}{2}$,实际上随便取什么都可以.\n",
    "\n",
    "令核函数为$K(X,Y)=(X^{T}Y+\\frac{1}{2})^{2}$\n",
    "\n",
    "有\n",
    "\n",
    "$K(X,Y)=(X^{T}Y+\\frac{1}{2})^{2}=(\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}^{T}\\begin{bmatrix}y_1\\\\ y_2\\end{bmatrix}+\\frac{1}{2})^{2}$\n",
    "\n",
    "$=(x_1y_1+x_2y_2+\\frac{1}{2})^2$\n",
    "\n",
    "$=x_1^2y_1^2+2x_1y_1x_2y_2+x_2^2y_2^2+x_1y_1+x_2y_2+\\frac{1}{4}$\n",
    "\n",
    "$=(x_1y_1+x_2y_2)^{2}+(x_1y_1+x_2y_2)+\\frac{1}{4}$\n",
    "\n",
    "$=[(x_1,x_2)\\begin{bmatrix}y_1\\\\ y_2\\end{bmatrix}]^{2}+(x_1,x_2)\\begin{bmatrix}y_1\\\\ y_2\\end{bmatrix}+\\frac{1}{4}$\n",
    "\n",
    "$=(X^{T}Y)^{2}+(X^{T}Y)+\\frac{1}{4}$\n",
    "\n",
    "$=<\\begin{pmatrix}X^2\\\\ X\\\\ \\frac{1}{2}\\end{pmatrix},\\begin{pmatrix}Y^2\\\\ Y\\\\ \\frac{1}{2}\\end{pmatrix}>$\n",
    "\n",
    "$=<H(X),H(Y)>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中,核函数在低维计算的结果完全等价于高维$K(X,Y)=(X^{T}Y+\\frac{1}{2})^{2}===<H(X),H(Y)>$\n",
    "\n",
    "这样就避开了高维空间中计算,但是核函数的形式不是固定的,只是上述使用的核函数正好适用于我们例子中的H(X),核函数的形式有以下几种:\n",
    "\n",
    "- 多项式核函数:\n",
    "    - $K(X,Y)=((X,Y)+R)^d$,上面的例子就是这个多项式核函数的一个特例即R=1/2,d=2\n",
    "    \n",
    "- 线性核函数:\n",
    "    - $K(X,Y)=<X,Y>$\n",
    "    \n",
    "- 高斯核函数:\n",
    "    - $K(X,Y)=exp\\begin{Bmatrix}-\\frac{||X-Y||^{2}}{2\\sigma^{2}}\\end{Bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中高斯核是用的最多的,通过控制参数$\\sigma$具有相当高的灵活性.也是使用最广泛的核函数之一.\n",
    "\n",
    "实际上只要满足[Mercer定理](https://en.wikipedia.org/wiki/Mercer%27s_theorem)的都可以作为核函数.有的核函数效果好,有的核函数效果不好.\n",
    "\n",
    "**核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。**(来自于v_JULY_v的博客)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**高斯核函数:**\n",
    "\n",
    "高斯核函数可以将原始空间映射为无穷维的空间上.不过如果$\\sigma$选的很大,那么高次特征上的权重实际上衰减的很快,所以实际上就相当于一个低微的子空间;反过来如果$\\sigma$选的很小,则可以在任意的数据映射为线性可分(也就是将数据分到更高维下进行超平面的划分),当然这也不是好事,因为随之而来的就是维数灾难,或者叫做严重过拟合问题。所以总的来说调整好参数$\\sigma$非常重要(来自于v_JULY_v的博客).\n",
    "![](picture/24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面举一个例子来说明SVM的非线性划分的效果(v_JULY_v):\n",
    "\n",
    "假设现在你是一个农场主，圈养了一批羊群，但为预防狼群袭击羊群，你需要搭建一个篱笆来把羊群围起来。但是篱笆应该建在哪里呢？你很可能需要依据牛群和狼群的位置建立一个“分类器”，比较下图这几种不同的分类器，我们可以看到SVM完成了一个很完美的解决方案。\n",
    "\n",
    "![](picture/25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们讨论了在假定线性可分的情况下的处理和线性不可分的情况下处理(将数据投射到高维空间划分超平面,但是由于维数灾难的存在,所以我们使用核函数在低维情况下进行计算高维。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.松弛变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们遇到如下的情况\n",
    "\n",
    "![](picture/26.png)\n",
    "\n",
    "那么像这种情况实际上我们是不需要进行非线性划分的,因为只有那么一两个数据点在另一个分类上(而且这些数据点很有可能是噪声),那么我们没有必要废大力气去进行高维度空间的转换,而是选择抛弃掉这些极个别的奇点或者叫噪声点.那么如何去抛弃这些噪声点呢?就需要使用**松弛变量**.\n",
    "![](picture/27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么下面开始讨论松弛变量\n",
    "\n",
    "我们原来的约束条件为:\n",
    "\n",
    "$y_i(w^{T}x_i+b) \\geqslant 1, i=1,...,n$\n",
    "\n",
    "现在考虑到处理松弛变量,则约束条件变为:\n",
    "\n",
    "$y_i(w^{T}x_i+b) \\geqslant 1-\\xi_i, i=1,...,n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\xi_i$就是松弛变量(slack variable),因为我们上述说到最近的距离也就是支持向量与超平面的距离假设为1，也就是说现在的假设最短距离就是1了，如果比1还小,那么就是跳到另一类中了.所以我们应该允许数据点$x_i$偏离一点超平面(函数间隔).当然$\\xi_i \\geqslant 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以原来的目标函数也应该更改为:\n",
    "\n",
    "$min \\frac{1}{2}||W^{T}||^2 + C\\sum_{i=1}^{n}\\xi_i$\n",
    "\n",
    "s.t. $y_i(W^{T}x_i+b)\\geqslant 1-\\xi,i=1,2..,n$\n",
    "\n",
    "$\\xi_i \\geqslant 0,i=1,...n$\n",
    "\n",
    "其中C是一个参数,是一个惩罚项用于控制目标函数中两项(寻找间隔最大的超平面和保证数据点偏差量最小)之间的权重.注意其中$\\xi$是需要优化的变量,而C是事先给定的一个常量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用之前的方法将限制条件加到目标函数中,构建一个新的拉格朗日函数:\n",
    "\n",
    "$F(w,b,\\xi,a,r)=\\frac{1}{2}||W^T||^2 + C\\sum_{i=1}^{n}\\xi_i-\\sum_{i=1}^{n}a_i\\cdot [y_i(W^{T}x_i+b)-1+\\xi_i]-\\sum_{i=1}^{n}r_i\\xi_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$r_i$是拉格朗日构造中的另一个$\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一样的方法将F对于$w,b,\\xi$求偏导最小化:\n",
    "\n",
    "$\\frac{\\partial F}{\\partial w}=0\\Rightarrow w=\\sum_{i=1}^{n}a_iy_ix_i$\n",
    "\n",
    "$\\frac{\\partial F}{\\partial b}=0\\Rightarrow \\sum_{i=1}^{n}a_iy_i=0 $\n",
    "\n",
    "$\\frac{\\partial F}{\\partial \\xi_i}=0\\Rightarrow C-a_i-r_i=0,i=1,...,n$\n",
    "\n",
    "然后再将其带回F中化简得到的结果和原来目标函数是一样的:\n",
    "\n",
    "$\\underset{a}{max}(\\sum_{i=1}^{n}a_i-\\frac{1}{2}\\sum_{i,j=1}^{n}a_ia_jy_iy_jx_i^{T}x_j)\\Rightarrow \\underset{a}{max}(\\sum_{i=1}^{n}a_i-\\frac{1}{2}\\sum_{i,j=1}^{n}a_ia_jy_iy_j<x_i,x_j>)$\n",
    "\n",
    "注意,由于$C-a_i-r_i=0,i=1,...,n$ 又有$r_i>0$,因此有$a_i\\leqslant C$,所以整个结论应该是\n",
    "\n",
    "$\\underset{a}{max}(\\sum_{i=1}^{n}a_i-\\frac{1}{2}\\sum_{i,j=1}^{n}a_ia_jy_iy_j<x_i,x_j>)$\n",
    "\n",
    "s.t., $0\\leqslant a_i \\leqslant C,i=1,...,n$\n",
    "\n",
    "$\\sum_{i=1}^{n}a_iy_i=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\">可以看到唯一的区别就是现在a多了一个上限C,而在非线性的情况下,我们只需要使用kernel(核函数)把$<x_i,x_j>$换成$k(x_i,x_j)$就可以了.这样一来一个完整的可以处理线性和非线性并且能容忍噪声的的支持向量机就完成了.</p>\n",
    "\n",
    "那么下面开始证明最难最SMO算法(实话说这个真的很难需要一些数学功底,真感叹当时从零开始创建新东西的时候那些牛逼的人真的是牛逼的不要不要的)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请跳转至5-2Support vector machines(SMO)查看\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
