{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture(Theory)\n",
    "\n",
    "这里我们将引入GMM并完成EM算法导出\n",
    "\n",
    "EM算法的一个重要应用是高斯混合模型的参数估计且在CV中的背景滤除具有重要的作用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Log-likelihood\n",
    "\n",
    "假设给定一组数据$X=\\{x_1,x_2...,x_N\\}$,其[极大似然估计](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)为:\n",
    "\n",
    "$L(\\theta|X)=log[P(X|\\theta)]=\\sum_{i=1}^{N}logP(x_i|\\theta)$\n",
    "\n",
    "很明显可以看出求一组数据的极大似然估计实际上是求该数据的后验概率:\n",
    "\n",
    "因为$P(\\theta|x) \\propto P(x|\\theta)P(\\theta)$.\n",
    "\n",
    "**Ps:**\n",
    "- 先验概率:知因求果.\n",
    "- 后验概率:知果求因.\n",
    "- 极大似然概率:知果求最可能的原因."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Singe Gaussian Model\n",
    "\n",
    "![](picture/40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果数据服从高斯分布,那么对于单个高斯分布,我们使用极大似然估计来求得$\\theta_1=\\{\\mu,\\Sigma\\}$是非常容易的.\n",
    "\n",
    "对于$\\theta_1=\\{\\mu,\\Sigma\\}:$\n",
    "\n",
    "$\\underset{\\theta}{argmax}[\\underset{L(\\mu,\\Sigma|X)}{\\underbrace{\\sum_{i=1}^{N}log \\displaystyle{N}(x_i|\\mu,\\sigma)}}]$\n",
    "\n",
    "$\\Rightarrow$\n",
    "\n",
    "$\\mu_{MLE}: \\frac{\\partial{L(\\mu,\\Sigma|X)}}{\\partial{\\mu}}=0\\Rightarrow\\frac{1}{N}\\sum_{i=1}^{N}x_i$\n",
    "\n",
    "$\\Sigma_{MLE}\\frac{\\partial{L(\\mu,\\Sigma|X)}}{\\partial{\\Sigma}}=0\\Rightarrow \\frac{\\sum_{i=1}^{N}(x_i -\\mu_{MLE})^2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Gaussian Mixture\n",
    "\n",
    "![](picture/41.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对于上述情况使用单个高斯拟合,那么拟合的结果如上图显示.很明显,这种拟合会有错误:\n",
    "- 对于数据集中度而言,高斯分布的数据应该大部分集中在$\\mu$附近,而上图却不是.\n",
    "- 上图很明显是两种类别,单个高斯无法做到很好的拟合\n",
    "\n",
    "所以对于多分类而言,使用单个高斯是无法进行数据拟合的,那么我们就要使用Gaussian Mixture,也就是多个高斯同时对数据集作用.\n",
    "\n",
    "![](picture/39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中每一个数据集都受到两个高斯的拟合,也就是说,在高斯混合模型中.每个数据样本均有来自不同高斯的概率影响,处于一种累加状态.所以GMM模型相比于K-means模型的好处是,GMM模型能够给予每个样本属于不同高斯的概率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Build GMM\n",
    "\n",
    "建立GMM,我们可以模仿single Gaussian进行.\n",
    "\n",
    "**Single Gaussian(vector):**\n",
    "\n",
    "$P(X|\\theta)=N(X|\\theta)={\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\exp \\left(-{\\frac {1}{2}}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\mathrm {T} }{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right),\\theta=\\{\\mu,\\Sigma\\},k=1,\\boldsymbol {\\Sigma }:real\\ number$\n",
    "\n",
    "$\\Theta_{MLE}=\\underset{\\theta}\\;{argmax}[L(\\theta|X)]=\\underset{\\theta}{argmax}\\; log[P(X|\\theta)]=\\underset{\\theta}{argmax}\\; \\sum_{i=1}^{N}log[P(x_i|\\theta)]=\\underset{\\theta}{argmax}[\\sum_{i=1}^{N}log N(x_i|\\theta)]$\n",
    "\n",
    "**Gaussian Mixture:**\n",
    "\n",
    "$P(X|\\theta)=\\sum_{l=1}^{K} \\alpha_l N(x_i|\\theta),\\sum_{l=1}^{K}\\alpha_l=1$\n",
    "\n",
    "${\\theta}=\\{\\mu_1,\\mu_2,...\\mu_k,\\Sigma_1,\\Sigma_2,...,\\Sigma_k,\\alpha_1,\\alpha_2,...,\\alpha_{k-1}\\}$\n",
    "\n",
    "$\\Theta_{MLE}=\\underset{\\theta}{argmax}\\;[L(\\theta|X)]=\\underset{\\theta}{argmax}\\; log[P(X|\\theta)]=\\underset{\\theta}{argmax}\\; \\sum_{i=1}^{N}log[P(x_i|\\theta)]=\\underset{\\theta}{argmax}\\;\\sum_{i=1}^{N}log[\\sum_{l=1}^{K}\\alpha_l N(x_i|\\theta)]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "- [多元正态分布](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)\n",
    "- k(k-mixture):K个高斯模型\n",
    "- $\\boldsymbol {\\Sigma }$:协方差矩阵,要是正半定和非奇异.\n",
    "- $P(X|\\theta)$的概率求和是1,由于是K个高斯混合,那么累加之后的概率不一定是1,所以需要放入一个权重(weights)$\\alpha_l$.\n",
    "- $\\alpha_{k-1}$:由于$\\sum_{l=1}^{N}\\alpha_l=1$,所以我们知道知道前K-1个就行了.\n",
    "- GMM实际上就是SG的累加,说明每一个样本都收到K个高斯的影响累加.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 求解GMM\n",
    "\n",
    "$\\Theta_{MLE}=\\underset{\\theta}{argmax}\\;\\{\\sum_{i=1}^{N}log[\\sum_{l=1}^{K}\\alpha_l N(x_i|\\theta)]\\}$\n",
    "\n",
    "正常情况下求解$\\Theta_{MLE}$,需要对${\\Theta}=\\{\\mu_1,\\mu_2,...\\mu_k,\\Sigma_1,\\Sigma_2,...,\\Sigma_k,\\alpha_1,\\alpha_2,...,\\alpha_{k-1}\\}$中的$\\mu_i,\\alpha_i,\\Sigma_i$分别求偏导并令导数等于0即可.但是这样做的复杂度很高.所以我们需要使用EM进行计算."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在GMM中,我们可以加入一个隐变量是关于K个混合模型的参数$Z$,那么就可以使用EM算法进行求解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 关于隐变量$Z$\n",
    "\n",
    "<img src=\"picture/43.png\"  height=\"500\" width=\"500\" />\n",
    "\n",
    "$z_{ik}=\\left\\{\\begin{matrix}\n",
    "1 &i\\;from\\; k\\; mixture \\\\ \n",
    "0 &  otherwise.\n",
    "\\end{matrix}\\right.$\n",
    "- 为什么在GMM中求解已经很复杂了,我们还要多加一个变量?\n",
    "    - 加入隐变量$Z$可以使得混合模型变为单个高斯模型.\n",
    "    \n",
    "- <p style=\"color:orange\">加入隐变量的前提是不能改变其边缘分布</p>\n",
    "\n",
    "    - $P(x_i)=\\int_{z_i}P(x_i|z_i)P(z_i)d_z=\\int_{z_i}P(z_i|x_i)d_z$,也就是说加入变量后,积分能够积掉,使其不改变边缘概率.(△)\n",
    "    - 比如在高斯模型中$P(x_i)$的边缘概率:\n",
    "    \n",
    "    $P(x_i)=\\int_{z_i}\\underset{N(x_i|\\mu_{z_i},\\Sigma_{z_i})} {\\underbrace{P(x_i|z_i)}} \\underset{\\alpha_{z_i}}{\\underbrace{P(z_i|\\theta)d_{z_i}}}=\\sum_{z_i=1}^{K}\\alpha_{z_i}N(x_i|\\mu_{z_i},\\Sigma_{z_i})$,结果和原始的高斯分布没有区别,所以不会改变$x_i$的边缘分布.这里的$\\int$变为$\\sum$是因为$z_i$是离散的.\n",
    "        - 实际上这里的$P(z_i|\\theta)$或者说$\\alpha_{z_i}$就是在完整数据下,不同数据属于第k个高斯模型的概率.比如图中,$x_2$属于第一个高斯还是第二个高斯的概率分别是多少?.\n",
    "        - 在迭代初始情况下一般都设置为$\\frac{1}{2}$\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 EM导出\n",
    "\n",
    "现在我们已经知道加入隐变量不会改变高斯的边缘概率,所以我们现在加入隐变量并假设我们已经知道了EM导出流程,那么GMM的$Q$函数为:\n",
    "\n",
    "Looking at the EM algorithm:\n",
    "\n",
    "$\\theta^{(g+1)}=\\underset{\\theta}{argmax}[Q(\\theta,\\theta^{(g)})]=\\underset{\\theta}{argmax}(\\sum_{Z} P(Z|X,\\theta^{(g)})\\;logP(X,Z|\\theta))=\\underset{\\theta}{argmax}\\;\\sum_{l=1}^{K}\\sum_{i=1}^{N}P(z|x_{i})ln \\alpha_l+\\sum_{l=1}^{K}\\sum_{i=1}^{N}P(z|x_{i})lnN(x_i|\\theta_l)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们建立完GMM的Q函数之后,我们的目标还是极大化不完全数据X关于$\\theta$的对数似然函数的估计值.\n",
    "\n",
    "那么为什么使用EM可以去近似极大似然估计呢,先看看一些基础知识."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Convex function\n",
    "\n",
    "设是定义在实数域上的函数，如果对于任意的实数，都有:\n",
    "\n",
    "$f'' \\ge0 \\\\ $\n",
    "\n",
    "那么是凸函数。若不是单个实数，而是由实数组成的向量，此时，如果函数的 Hesse 矩阵是半正定的，即\n",
    "\n",
    "$H'' \\ge 0 \\\\ $\n",
    "\n",
    "是凸函数.\n",
    "\n",
    "特别地，如果 $f'' > 0$ 或者  $H'' > 0$ ，称为严格凸函数.\n",
    "\n",
    "**Ps:**\n",
    "- 凸函数可以求得全局最优解,比如一个二次函数.\n",
    "- 非凸函数可以求得局部最优解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Expectation\n",
    "\n",
    "对于离散型随机变量$X$的概率分为为$p_i=p\\{X=x_i\\}$,数学期望$E(X)$为:\n",
    "\n",
    "$E(X)=\\underset{i}{\\sum}x_ip_i$\n",
    "\n",
    "$p_i$是权值(概率),满足条件:$1. 1\\geqslant p_i\\geqslant 0,\\underset{i}{\\sum}p_i=1$\n",
    "\n",
    "若连续型随机变量X的概率密度函数为$f(x)$,则数学期望$E(x)$为:\n",
    "\n",
    "$E(x)=\\int_{-\\infty }^{+\\infty}xf(x)d_{x}$\n",
    "\n",
    "若$Y=g(X)$,若$X$是离散型随机变量,则:\n",
    "\n",
    "$E(Y)=\\underset{i}{\\sum}g(x_i)p_i$\n",
    "\n",
    "若$X$是连续型随机变量,则:\n",
    "\n",
    "$E(Y)=\\int_{-\\infty }^{+\\infty}g(x)f(x)d_{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Jensen不等式\n",
    "\n",
    "如下图,如果函数$f$是凸函数,$x$是随机变量,有0.5的概率是a,有0.5的概率是b,$x$的期望就是a和b的中值:\n",
    "\n",
    "$E[f(x)]\\geqslant f(E[x])$\n",
    "\n",
    "其中,$E[f(x)]=0.5f(a)+0.5f(b)=0.5(f(a)+f(b)),f(E(x))=f(0.5a+0.5b)=f(0.5(a+b))$,这里a和b的权值为0.5,$f(a)$与a权值相等,$f(b)$与b的权值相等.\n",
    "\n",
    "特别的,如果函数$f$是严格凸函数,当且仅当:$p(x=E(x))=1$(即随机变量是常量)时等号成立.\n",
    "\n",
    "![](picture/44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "- 若函数$f$是凹函数,Jensen不等式符号相反.\n",
    "- 一句话Jensen表达的就是:在凸函数中,无论在何区间取值,虚线上点的值要大于函数$f(x)$的值,凹函数同理."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 EM的推导流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在GMM中,将观测数据$X=(x_1,x_2,...,x_N)$,不可观测数据(GMM中表示属不属于某一个k高斯模型)$Z=(z_1,z_2,...,z_N)$,将$X,Z$合在一起称为完全数据.那么**观测数据**的似然函数为:\n",
    "\n",
    "$l(\\theta)=\\prod_{j=1}^{N}p(x_j|\\theta)=\\prod_{j=1}^{N}\\underset{z}{\\sum}P(z|\\theta)P(x_j|z,\\theta)$\n",
    "\n",
    "其中求和号表示对$z$的所有可能求和.\n",
    "\n",
    "为了推导方便,我们将$\\prod$省去,因为我们是要证明EM是可以通过迭代近似极大似然估计值,既然是极大,那么少一个$\\prod$对结果是没有影响的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么对数似然函数为:\n",
    "\n",
    "$L(\\theta)=lnP(X|\\theta)=ln \\underset{z}{\\sum}P(z|\\theta)P(X|z,\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM算法是一种迭代算法,通过迭代的方法求取目标函数$L(\\theta)=lnP(X|\\theta)$的极大值.**因此,希望每一步迭代之后的目标函数会比上一步迭代之后的值要大.设当前第n次迭代后参数的取值为$\\theta_n$,我们的目标是使得$L(\\theta_{n+1}) > L(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么考虑:\n",
    "\n",
    "$L(\\theta)-L(\\theta_{n})=ln(\\underset{z}{\\sum}P(z|\\theta)P(X|z,\\theta))-lnP(X|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道Jensen不等式(凸函数):\n",
    "\n",
    "$E[f(x)]\\geqslant f(E(x))$\n",
    "\n",
    "也就是:\n",
    "\n",
    "$\\sum_{j}\\lambda_{j}f(x_j) \\geqslant f(\\sum_{j}\\lambda_{j} x_j)$,其中$\\lambda_{j}>0,\\sum_{j}\\lambda_{j}=1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为$\\underset{z}{\\sum}P(z|X,\\theta_n)=1$(加入的隐变量不能改变边缘概率,所以积分或者求和后要等于1,可以回顾(△)部分),所以对于$L(\\theta)-L(\\theta_{n})$的第一项等价变形以及Jensen不等式有:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ln(\\underset{z}{\\sum}P(z|\\theta)P(X|z,\\theta))=\\underset{f(E(X))}{ \\underbrace {ln(\\underset{z}{\\sum}P(z|X,\\theta_n)\\cdot \\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta_n)}}}) \\geqslant \\underset{E(f(X))}{ \\underbrace{  \\underset{z}{\\sum}P(z|X,\\theta_n) ln(\\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta_n)}}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为$ln$函数是凹函数,所以不等式成立.\n",
    "\n",
    "**Ps:**\n",
    "- $f(x)$:对数函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二项有:\n",
    "\n",
    "$−lnP(X|θ_n)=−∑_zP(z|X,θ_n)\\cdot lnP(X|θ_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则$L(\\theta)-L(\\theta_n)$下界为:\n",
    "\n",
    "$L(\\theta)-L(\\theta_n)\\geqslant \\underset{z}{\\sum}P(z|X,\\theta_n)\\cdot ln\\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta)}-\\underset{z}{\\sum}P(z|X,\\theta_n)lnP(X|\\theta_n)$\n",
    "\n",
    "$=\\underset{z}{\\sum}[P(z|X,\\theta_n)ln\\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta_n)}-P(z|X,\\theta_n)lnP(X|\\theta)]$\n",
    "\n",
    "$=\\underset{z}{\\sum}P(z|X,\\theta_n)ln\\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta_n)P(X|\\theta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数$l(\\theta|\\theta_n)$:\n",
    "\n",
    "$l(\\theta|\\theta_n)\\doteq L(\\theta_n)+\\underset{z}{\\sum}P(z|X,\\theta_n)ln\\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta_n)P(X|\\theta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从而有$L(\\theta)\\geqslant l(\\theta|\\theta_n)$,也就是说第n次迭代后,$L(\\theta)$的一个下界为$l(\\theta|\\theta_n)$.另外有等式$L(\\theta_n)=l(\\theta|\\theta_n)$成立."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:orange\">那么现在我们的目标$L(\\theta_{n+1})>L(\\theta_n)$转换为$L(\\theta_{n+1})> l(\\theta_n|\\theta_n)$,那么由于$l(\\theta_n|\\theta_n)$是$L(\\theta)$的下界,所以任何能够让$l(\\theta_n|\\theta_n)$增大的$\\theta$,也可以让$L(\\theta)$增大.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过下图可以解释:\n",
    "![](picture/45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下界的曲线当然是随着迭代的进行而变化的:在第$i$次迭代结束后，总是有不等式$L(\\theta)≥l(\\theta|\\theta_i)$ 和等式 $L(\\theta_i)=l(\\theta_i|\\theta_i)$成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:orange\">也就是说EM算法通过优化对数似然$L(\\theta)$的下界$l(\\theta|\\theta_i)$,来间接优化$L(\\theta)$的.</p>\n",
    "\n",
    "**Ps:**\n",
    "- 图中可以看出找到的最优$\\theta$不一定是全局最优解,收到初始值$\\theta_0$的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么最终问题就变成了如何找到这个最优的$\\theta$在$l(\\theta|\\theta_i)$中.\n",
    "\n",
    "所以$l(\\theta|\\theta_i)$的最优解:\n",
    "\n",
    "$\\theta_{n+1}=\\underset{\\theta}{argmax}\\;l(\\theta|\\theta_n)$\n",
    "\n",
    "因为是寻找极大的$\\theta$,所以剔除与$\\theta$无关的变量,从而有:\n",
    "\n",
    "$\\theta_{n+1}=\\underset{\\theta}{argmax}\\;l(\\theta|\\theta_n)\\doteq \\underset{\\theta}{argmax}\\; \\{L(\\theta_n)+\\underset{z}{\\sum}P(z|X,\\theta_n)ln\\frac{P(z|\\theta)P(X|z,\\theta)}{P(z|X,\\theta_n)P(X|\\theta_n)}\\}$\n",
    "\n",
    "$=\\underset{\\theta}{argmax}\\;\\underset{z}{\\sum}P(z|X,\\theta_n)lnP(z|\\theta)P(X|z,\\theta) $\n",
    "\n",
    "$=\\underset{\\theta}{argmax}\\;\\underset{z}{\\sum}P(z|X,\\theta_n)lnP(X,z|\\theta)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ps:**\n",
    "- 上面的$L(\\theta_n)$剔除是以为迭代的上一步的结果是已知的为一个常数.\n",
    "- $ln$中的分母中是与$\\theta$无关的量,所以也无需理会."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后定义最终结果为一个Q函数:\n",
    "\n",
    "$\\theta_{n+1}=\\underset{\\theta}{argmax}\\;\\underset{z}{\\sum}P(z|X,\\theta_n)lnP(X,z|\\theta)=\\underset{\\theta}{argmax}\\;Q(\\theta|\\theta_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里,我们也就知道了为什么EM能够近似的估计极大似然值,因为EM是通过下界的迭代来间接优化极大似然的值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 EM算法\n",
    "\n",
    "EM算法:\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "使用$Q$函数对各个参数求偏导数的时候,偏导数是与后验概率相关的,所以在写代码时,EM算法的E步,我们会从后验概率的表达式计算开始\n",
    "\n",
    "输入:观测变量数据X,隐变量数据Z,联合分布$P(X,Z|\\theta)$,条件分布(后验概率)$P(Z|X,\\theta)$;\n",
    "输出:模型参数$\\theta$\n",
    "\n",
    "(1) 选择初始参数进行迭代\n",
    "\n",
    "(2) E步: 从后验概率开始计算出各个参数所需要的值.\n",
    "\n",
    "(3) M步: 使得$Q(\\theta,\\theta^{g})$极大化$\\theta$\n",
    "\n",
    "$\\theta^{(g+1)} = \\underset{\\theta} argmax\\;Q(\\theta,\\theta^{(g)})$\n",
    "\n",
    "(4) 不断迭代(2),(3)直到收敛,收敛的条件是可以给予一个较小的阈值$\\varepsilon$使得,\n",
    "\n",
    "$||Q^{(g+1)}-Q^{(g)}||\\leqslant \\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 EM算法收敛性定理\n",
    "\n",
    "定理1: 观测数据的似然函数为$P(X|\\theta)$通过EM算法得到的序列$P(X|\\theta_n)$单调递增:$P(X|\\theta)\\geqslant P(X|\\theta_n)$\n",
    "\n",
    "定理2:\n",
    "- (1)如果$P(X|\\theta)$有上界,则$L(\\theta_n)=lnP(X|\\theta_n)$收敛到某一值$L^{*}$\n",
    "- (2)在$Q$函数与$L(\\theta)$满足一定条件下,EM算法得到的收敛值$\\theta^{*}$是$L(\\theta)$的稳定点,所以由EM算法得出的$\\theta$不一定是全局最优解,很大肯能是局部最优解.\n",
    "- (3)EM算法受初值的影响较大\n",
    "\n",
    "**Ps:**\n",
    "- 使用EM算法一定要知道数据的分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 EM-GMM\n",
    "\n",
    "现在我们已经知道了EM的流程.那么将EM应用于Gaussian Mixture中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设观测数据$X=(x_1,x_2,...,x_N)$由K个高斯混合模型生成,\n",
    "\n",
    "$P(X|\\theta)=\\sum_{k=1}^{K} \\alpha_l N(x_i|\\theta),\\sum_{l=1}^{K}\\alpha_l=1$\n",
    "\n",
    "${\\theta}=\\{\\mu_1,\\mu_2,...\\mu_k,\\Sigma_1,\\Sigma_2,...,\\Sigma_k,\\alpha_1,\\alpha_2,...,\\alpha_{k-1}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐变量$z_{ik}$:\n",
    "\n",
    "$z_{ik}=\\left\\{\\begin{matrix}\n",
    "1 &i\\;from\\; k\\; mixture \\\\ \n",
    "0 &  otherwise.\n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E-setp:\n",
    "\n",
    "计算$Q(\\theta|\\theta^{(g)}):$\n",
    "\n",
    "$Q(\\theta|\\theta^{(g)})=\\underset{z}{\\sum}P(z|X,\\theta_k)lnP(X,z|\\theta_k)$\n",
    "\n",
    "$=\\sum_{i=1}^{N}\\{\\underset{z_{ik}}{\\sum}P(z_{ik}|x_i,\\theta_k)lnP(x_i,z_{ik}|\\theta_k)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对所有的$\\underset{z_{ik}}{\\sum}$的可能求和即为:\n",
    "\n",
    "$=\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})lnP(x_i,z_{ik}|\\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以体现为每一个数据$i$对应的隐变量$z$受不同k高斯模型的影响."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步先求$P(z_{ik}|x_i)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由贝叶斯概率以及联合概率可以知道后验概率$P(z_{ik}|x_i,\\theta_k)$为:\n",
    "\n",
    "$P(z_{ik}|x_i,\\theta_k)=P(z_{ik}=1|x_i,\\theta_k)$\n",
    "\n",
    "$=\\frac{P(z_{ik}=1,x_i|\\theta_k)}{\\sum_{k=1}^{K}P(z_{ik}=1,x_i|\\theta_k)}$\n",
    "\n",
    "$=\\frac{P(x_i|z_{ik}=1,\\theta_k)P(z_{ik}=1|\\theta_k)}{\\sum_{k}^{K}P(x_i|z_{ik}=1,\\theta_k)P(z_{ik}=1|\\theta_k)}$\n",
    "\n",
    "$=\\frac{\\alpha_k N(x_i|\\theta_k)}{\\sum_{k}^{K}\\alpha_k N(x_i|\\theta_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(z_{ik}|x_i,\\theta_k)$称为观测数据$x_i$的响应度(response probability):第i个观测数据来自第k个高斯模型的概率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再求$P(x_i,z_{ik}|\\theta_{k})$:\n",
    "\n",
    "$P(x_i,z_{ik}|\\theta_{k})=P(z_{ik}|\\theta_k)P(x_i|z_{ik},\\theta_k)$\n",
    "\n",
    "$=\\alpha_kN(x_i|\\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以GMM的Q函数为:\n",
    "\n",
    "$Q(\\theta|\\theta^{(g)})=\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})lnP(x_i,z_{ik}|\\theta_k)$\n",
    "\n",
    "$=\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})ln\\alpha_kN(x_i|\\theta_k)$\n",
    "\n",
    "把$P(z_{ik}|x_i,\\theta_{k})$放到$ln$中去\n",
    "\n",
    "$=\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})ln\\alpha_k +\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})lnN(x_i|\\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M-step:\n",
    "\n",
    "得到Q函数了之后可以对$\\alpha,\\theta=\\{\\mu,\\Sigma\\}$求偏导并令其等于0.\n",
    "\n",
    "(1) 求解$\\alpha$需要使用Lagrange Multiplier 得到的结果是:\n",
    "\n",
    "$\\alpha_k=\\frac{\\sum_{i=1}^{N}P(z_{ik}|x_i,\\theta_k)}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 求解$\\mu,\\Sigma$需要了解矩阵求导:\n",
    "\n",
    "$\\mu_k=\\frac{\\sum_{i=1}^{N}P(z_{ik}|x_i,\\theta_k)x_i}{\\sum_{i=1}^{N}P(z_{ik}|x_i,\\theta_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma_{K}=\\frac{\\sum_{i=1}^{N}[x_i-\\mu_k][x_i-\\mu_k]^T P(z_{ik}|x_i,\\theta_k)}{\\sum_{i=1}^{N}P(z_{ik}|x_i,\\theta_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Summary:\n",
    "- 这里的$\\Sigma$需要注意,分子中的转置相乘实际上是使用外积,否则的出来的矩阵是不满足半正定条件的.\n",
    "- 在使用GMM的时候E步直接求后验概率$P(z_{ik}|x_i,\\theta_k)$来更新参数.\n",
    "- 可以设置一个Loss function为KL指标,来衡量模型的好坏.\n",
    "    - ![](picture/46.png)\n",
    "- 理论上$\\Sigma$不能奇异,但是在迭代过程中肯定会产生一些奇异矩阵,所以一般在计算多元正态函数时,我们是允许奇异的.\n",
    "- $\\alpha_k$一定程度上可以看成某一个数据$i$是属于$k$高斯模型的权重.\n",
    "- 更多M-step见[GMM模型详推导](http://crescentmoon.info/2013/04/02/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
