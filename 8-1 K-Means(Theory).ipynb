{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "K均值(K-Means)算法是无监督的聚类方法,实现起来比较简单,聚类效果也比较好,因此应用很广泛.另外K-Means算法也可以看成是GMM的一个特例."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 K-means\n",
    "\n",
    "比如我们现在有一组数据(为了说明方便,我们在阐述理论的时候只使用2D数据)如下图所示\n",
    "<img src=\"picture/48.png\" width=\"300px\" heigth=\"300\">\n",
    "现在我们要将这些数据分为2类,很直观的可以看出,可以分为左上和右下."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么K-mean就是解决这个问题的一个办法,具体是:\n",
    "\n",
    "我们考虑寻找多维空间中数据点的分组和聚类的问题.假设我们现在有一个数据集${x_1,x_2,...,x_N}$.我们的目标是将数据划分为K类.在直观上讲,我们会认为由一组数据构成的一个类中,**聚类的内部点之间的距离应该会很近且与其他类内点的距离会远**.我们引入一组D维的向量$c_k$,其中$k=1,2,...,K$,$c_k$就是第$k$个聚类的中心点.所以我们的目标就是:\n",
    "\n",
    "找到数据点分别属于的聚类,使得每一个数据点个与它最近的向量$c_k$之间的距离平方和最小.\n",
    "\n",
    "目标函数:\n",
    "\n",
    "$J= {argmin}\\;||x_i - c_k||^{2}$\n",
    "\n",
    "**Ps:**\n",
    "- 距离的度量方式有很多,可以去KNN中查看,在K-means中使用最多的是欧氏距离.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 K-Means Algorithm\n",
    "\n",
    "K-Means Algorithm:\n",
    "\n",
    "输入:样本集$X={x_1,x_2,...,x_m}$,聚类数为$K$,最大迭代数$Iter$.\n",
    "\n",
    "输出: 划分的聚类数$c_1,c_2,...,c_k$\n",
    "\n",
    "(1)从数据集中随意挑选出$K$个样本作为初始的质心(center)向量$\\mu$\n",
    "\n",
    "(2)迭代$iter=1,2,...,Iter$.\n",
    "\n",
    "- 对于$i=1,2,...,m;k=1,2,...,K$个样本$x_i$计算和各个center的距离$d_{ik}$ \n",
    "    - $d_{ik}= ||x_{ik}-\\mu_{ik}||^{2}$或者$D = \\sum_{i=1}^{m}\\sum_{k=1}^{K}||x_i-\\mu{k}||^{2}$\n",
    "    \n",
    "- 对于每一个样本点,获取与其最近的center,将其归为一类.\n",
    "\n",
    "- 得到新的$k$类群后,计算新的center,即当前聚类群的均值.\n",
    "    - $\\mu=\\frac{\\sum x_{k}}{N_{c_k}}$\n",
    "    \n",
    "- 如果$k$个center向量变化在阈值内,则视为收敛退出迭代\n",
    "\n",
    "(3) 得到$K$个center:$C={c_1,c_2,...,c_k}$\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "- 在迭代中划分新的$k$类群中,如果出现某一个$k$center没有任何数据,那么我们可以选择抛弃这个center,在实际过程中一般不会发生.\n",
    "- 对于初始$K$值的选择,我认为应该是**依靠对于目的的需求**.\n",
    "- 对于初始$K$center的选取,如果是完全随机的话,那么效果并不会很理想,所以我们会使用**K-means++**来选择初始center\n",
    "\n",
    "对于$K$的选取,个人认为主要还是依照需求而言,如果使用类似于\"Elbow method\"来选取初始值$K$,在实际情况中基本是无效的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 直观表达\n",
    "\n",
    "![](picture/49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于数据样本(a)做二分类,初始值随机选取两个点(b),使用算法(2)最后得到的结果就是(f).\n",
    "\n",
    "- 如果对于算法不是很清楚,可以查看Application中的代码示例."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 K-Means ++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[K-Means++ paper](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)\n",
    "\n",
    "在传统的K-Means算法中,如果初始值的选择纯随机的话,可能收敛会很慢,并且效果也不会好.那么我们现在引入K-means++\n",
    "\n",
    "K-means++算法的选择初始seeds的基本思路是:初始的聚类中心之间的相互距离要尽可能的远,这一点和SVM的思想是有点相似的,比如在二分类中,随机的初始seeds正好是两个极为临近的点,那么分类效果肯定不好.\n",
    "\n",
    "**K-Means++ algorithm:**\n",
    "\n",
    "输入:数据集,聚类数量$K$\n",
    "\n",
    "输出: 聚类center$c_1,c_2,...,c_K$\n",
    "\n",
    "(1)从输入的数据点集合中随机选择一个点作为第一个聚类center.\n",
    "\n",
    "(2)对于样本的每一个数据点,**计算与其最近的center**,得到距离$d_i$,center集群中的样本组成的距离向量为$D(X)=\\{d_1,d_2,...,d_{n}\\}$.\n",
    "\n",
    "(3) 以概率的方式选择距离较大的点作为下一个聚类center.\n",
    "\n",
    "(4)重复(2),(3)直到获取$K$个center作为初始center放入传统的K-Means中去.\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "对于K-Means++算法中较难的就是(3)何为以概率的方式选择距离最大的点,为什么要以概率最大的方式?\n",
    "\n",
    "- 为什么要以概率最大的方式:\n",
    "    - 由于数据中可能存在噪声(奇异点),那么这个点距离center肯定最远,如果只是单纯的选取距离最大的点,那么很可能会选到噪声点.\n",
    "    \n",
    "- 何为以概率的方式:\n",
    "    - 正是由于噪声点的存在,所以我们需要以概率的方式:\n",
    "    \n",
    "考虑如下图:\n",
    "\n",
    "![](picture/50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$A,B,C,D$是距离$center_1$最近的4个点,那么其组成$D(X)$,如上图所示:当取值$\\sum D(x)\\cdot random$ 时,得到的值会以较大概率落入20这个区间内.所以这个办法就可以替我们选出下一个距离最远的center,具体做法如下:\n",
    "\n",
    "随机取一个Random能够落在$\\sum D$内,然后做循环: Random -= $D(x)$ 直到Random <=0,的时候表示找到了这个概率最大的点.直观的理解是:\n",
    "\n",
    "<p style=\"color:orange\">如果这个值以较大概率落在距离大的区间内,那么这个Random肯定要概率大于其他区间小的$d_i$,所以在循环自减$d_i$的时候,一旦小于0,即表示这个Random不够跨过当前这个长度区间了,所以此时的值就是样本点距离center概率最远的点.</p>\n",
    "\n",
    "比如以上面的例子举例:\n",
    "\n",
    "$\\sum D=43$,随机的random = 0.5,那么随机数Random = 43 * 0.5 = 21.5.循环自减的时候,第一个10或者5或者8减掉之后再减20的时候就会发现Random<0,那么找到的这个点也就是当前距离最大的点了.\n",
    "\n",
    "**PS:**\n",
    "\n",
    "对应在代码中的**KPP function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Other\n",
    "\n",
    "对于K-Means的距离优化,以及Mini batch就不在这里提及了,因为对于大样本而言,K-Means是不使用的,当K达到100以上是,计算是非常耗时的,而且效果也不会好,我们可以使用CTC也就是神经网络."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.K—Means and GMM\n",
    "\n",
    "其实对比高斯模型的EM算法和K-Means算法,两者很相似,不同的是K-Means对数据点进行了\"硬分配\",也就是说数据点只属于某个聚类$k$,而GMM却能给出每一个数据点属于某个$k$高斯的概率值,这一点体现在后验分布$P(Z|X,\\theta)$上.\n",
    "\n",
    "$z\\in \\{1,2,...,K\\}$为隐变量\n",
    "\n",
    "那么对于一个GMM模型:\n",
    "\n",
    "$P(X|\\theta)=N(X|\\theta)=\\sum_{n=1}^{N}\\sum_{k=1}^{K}{\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\exp \\left(-{\\frac {1}{2\\sqrt{|\\Sigma|}}}(||{\\mathbf {x} }-{\\boldsymbol {\\mu }}||^{2})\\right),\\theta=\\{\\mu,\\Sigma\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于其后验概率:\n",
    "\n",
    "$P(z_{ik}|x_i,\\theta_k)=\\frac{\\alpha_k {\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\exp \\left(-{\\frac {1}{2\\sqrt{|\\Sigma|}}}(||{\\mathbf {x} }-{\\boldsymbol {\\mu }}||^{2})\\right)}{\\sum_{k}^{K}\\alpha_k {\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\exp \\left(-{\\frac {1}{2\\sqrt{|\\Sigma|}}}(||{\\mathbf {x} }-{\\boldsymbol {\\mu }}||^{2})\\right)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来分析为什么K-means会是GMM的一个特例:\n",
    "\n",
    "现在分子分母都只考虑$exp$部分(收敛速度最快):\n",
    "\n",
    "$\\exp \\left(-{\\frac {1}{2\\sqrt{|\\Sigma|} }}(||{\\mathbf {x} }-{\\boldsymbol {\\mu }}||^{2})\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$\\Sigma \\rightarrow 0$:\n",
    "\n",
    "(1)分子部分:意味着第$i$的样本点$x_i$与第$k$个高斯模型之间的关系很弱,因为[协方差](https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE)接近于0.体现为这个样本点$x_i$不属于这个高斯模型或者说和这个高斯模型关系不大.所以整体的$exp$接近0.\n",
    "\n",
    "(2)分母部分:每一个样本点$x_i$是有对应一个高斯模型,记为$k_j$,那么实际上除了第$j$项,其他由于协方差接近0,整体$exp$接近0.\n",
    "\n",
    "所以综合(1),(2):\n",
    "\n",
    "a.当某一样本点与第$k$个高斯无关的时候,其后验概率$P(z_{ik}|x_i,\\theta_k)$为0.\n",
    "\n",
    "b.当某一样本点与第$k$个高斯有关的时候,其后验概率$P(z_{ik}|x_i,\\theta_k)$接近1.\n",
    "\n",
    "\n",
    "结合a,b:\n",
    "\n",
    "在K-Means中体现了一个样本点只属于某一聚类$k$\n",
    "\n",
    "**Ps:**\n",
    "\n",
    "协方差可以体现相关性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以后验概率可以写为:\n",
    "\n",
    "$r_{nk}= \\left\\{\\begin{matrix}\n",
    "1 &if \\; k=argmin||x_n-\\mu_k||^2 \\\\  0\n",
    " & other.\n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以由GMM的Q函数演变为K-means的Q函数如下:\n",
    "\n",
    "E-step:\n",
    "\n",
    "$Q^{prev}=\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})ln\\alpha_k +\\sum_{i=1}^{N}\\sum_{k=1}^{K}P(z_{ik}|x_i,\\theta_{k})ln \\{ {\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\exp \\left(-{\\frac {1}{2\\sqrt{|\\Sigma|}}}(||{\\mathbf {x} }-{\\boldsymbol {\\mu }}||^{2})\\right)\\}$\n",
    "\n",
    "$\\Rightarrow$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=\\sum_{i=1}^{N}\\sum_{k=1}^{K}r_{nk}ln\\alpha_k +\\sum_{i=1}^{N}\\sum_{k=1}^{K}r_{nk}ln \\{ {\\frac {1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma }}|}}}\\exp \\left(-{\\frac {1}{2\\sqrt{|\\Sigma|}}}(||{\\mathbf {x} }-{\\boldsymbol {\\mu }}||^{2})\\right)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过整理,可以推出对数似然函数形式为:\n",
    "\n",
    "$\\Rightarrow$\n",
    "\n",
    "$=const  -\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||x-\\mu||^{2}$\n",
    "\n",
    "后半部分实际上就是损失函数$J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们的目标是$\\mu$极大化,所以我们可以抛出一些常数项,并且要极大似然函数,那么就相当于极小$\\sum_{n=1}^{N}\\sum_{k=1}^{K}||x-\\mu||^{2}$:\n",
    "\n",
    "$Q=\\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||x_i-\\mu_k||^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M-step:\n",
    "\n",
    "寻找$\\mu_k$使得$Q_k$最小就相当于$\\mu$的极大:\n",
    "\n",
    "那么对Q求$\\mu$的偏导:\n",
    "\n",
    "$\\mu_k= \\frac{\\sum_n r_{nk}x_n}{\\sum_n r_{nk}} = \\frac{\\sum x}{N_{k}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Summary:\n",
    "\n",
    "- K-Means是属于非监督学习的一种,所以整体算法过程没有label的参与.\n",
    "- 在选择初始值的时候需要使用K-Means++的方法计算出初始迭代值\n",
    "- K-Means是GMM的一种特殊情况,当GMM中有K个高斯进行拟合的时候,每一个数据点只与一个高斯相关,这就是为什么说K-Means是GMM的特例."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
