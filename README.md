# Classic machine learning
<figure class="third">
    <img src="picture/heyra.png" width="50" heigth="50"/>
</figure>

这里是我汇集总结的经典机器学习十大算法.

Ps:

在总结的过程中,查询和引用了很多资料,以至于有些来源尚未注明,如果大家发现需要我重新注明来源欢迎发邮件提醒,我将尽快注明.

邮箱地址:jokermonster030@gmail.com

如果发现其中有问题的,欢迎issue.

- [KNN](https://github.com/woaij100/Classic_machine_learning/blob/master/1-K%20nearest%20neighbor.ipynb)
- [Decision Trees](https://github.com/woaij100/Classic_machine_learning/blob/master/2-Decision%20Trees.ipynb)
- [Naive Bayes(Theory)](https://github.com/woaij100/Classic_machine_learning/blob/master/3-1Naive%20Bayes(Theory).ipynb)
- [Naive Bayes(Application)](https://github.com/woaij100/Classic_machine_learning/blob/master/3-2Naive%20Bayes(Application).ipynb)
- [Naive Bayes(Gaussian)](https://github.com/woaij100/Classic_machine_learning/blob/master/3-3%20Naive%20Bayes(Gaussian).ipynb)
- [Logistic Regression(Theory01)](https://github.com/woaij100/Classic_machine_learning/blob/master/4-1%20Logistic%20Regression(Theory01).ipynb)
- [Logistic Regression(Theory02)](https://github.com/woaij100/Classic_machine_learning/blob/master/4-2%20Logistic%20Regression(Theory02).ipynb)
- [Logistic regression(Application)](https://github.com/woaij100/Classic_machine_learning/blob/master/4-3%20Logistic%20regression(Application).ipynb)
- [Softmax(Theory)](https://github.com/woaij100/Classic_machine_learning/blob/master/4-4%20Softmax(Theory).ipynb)
- [Softmax(Application)](https://github.com/woaij100/Classic_machine_learning/blob/master/4-5%20Softmax(Application).ipynb)
- [Support vector machines(Theory)](https://github.com/woaij100/Classic_machine_learning/blob/master/5-1Support%20vector%20machines(Theory).ipynb)
- [Support vector machines(SMO)](https://github.com/woaij100/Classic_machine_learning/blob/master/5-2Support%20vector%20machines(SMO).ipynb)
- [Support vector machines(Application01)](https://github.com/woaij100/Classic_machine_learning/blob/master/5-3%20Support%20vector%20machines(Application01).ipynb)
- [AdaBoost](https://github.com/woaij100/Classic_machine_learning/blob/master/6-AdaBoost.ipynb)
- [EM](https://github.com/woaij100/Classic_machine_learning/blob/master/7-1EM.ipynb)
- [Gaussian Mixture(Theory)](https://github.com/woaij100/Classic_machine_learning/blob/master/7-2Gaussian%20Mixture(Theory).ipynb)
- [Gaussian Mixture Model(Application)](https://github.com/woaij100/Classic_machine_learning/blob/master/7-3Gaussian%20Mixture%20Model(Application).ipynb)
- [K-Means(Theory)](https://github.com/woaij100/Classic_machine_learning/blob/master/8-1%20K-Means(Theory).ipynb)
- [K-Means(Application)](https://github.com/woaij100/Classic_machine_learning/blob/master/8-2%20K-Means(Application).ipynb)



其中对于EM计算HMM(Hidden Markove Model)我没有将其归纳,由于现在都是使用CTC居多.所以来日我会在DeepLearn 中详细说明HMM转CTC在NLP方面的应用.

Good Luck ~~~

​																				——— By Joker.
